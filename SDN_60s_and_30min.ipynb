{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "All computational and statistics packages have been tested in Anaconda environment (python 3.6)\n",
    "1. Install Anaconda\n",
    "https://www.anaconda.com/download/#linux\n",
    "2. Install Spark and Java 8\n",
    "https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Install \"pyspark\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Trying to import pyspark...')\n",
    "    import pyspark\n",
    "except ImportError:\n",
    "    print('Pyspark import failed...')\n",
    "    print('Installing pyspark in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create Spark Context***\n",
    "If you enqounter any error, try to reconfigure Spark on your machine (link in the first cell - .bashrc file configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    conf = SparkConf().setAppName('SDN')\n",
    "    sc = SparkContext(conf=conf)\n",
    "else:\n",
    "    if sc != None:\n",
    "        sc.stop()\n",
    "    conf = SparkConf().setAppName('SDN')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkConf, SparkContext\n",
    "# conf = SparkConf().setAppName('SDN').setMaster('spark://10.0.2.15:7077').setSparkHome('/usr/local/spark/') \n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SQL context for spark computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import scipy.stats as sts\n",
    "\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.triggers.interval import IntervalTrigger\n",
    "import matplotlib.dates as mdates\n",
    "import json\n",
    "import urllib\n",
    "import logging, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramiko installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Trying to import paramiko...')\n",
    "    import paramiko\n",
    "    print('Paramiko imported.')\n",
    "except ImportError:\n",
    "    print('Paramiko import failed...')\n",
    "    print('Installing paramiko in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} paramiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCP installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Trying to import scp...')\n",
    "    import scp\n",
    "    print('Scp imported.')\n",
    "except ImportError:\n",
    "    print('Scp import failed...')\n",
    "    print('Installing scp in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Trying to import keras...')\n",
    "    import keras\n",
    "    print('Keras imported.')\n",
    "except ImportError:\n",
    "    print('Keras import failed...')\n",
    "    print('Installing keras in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loginanddownload(hostname,uname,pwd,sfile,tfile):\n",
    "    \"\"\"\n",
    "    Can copy files and directories from PNDa to remote system.\n",
    "    Usage example:\n",
    "        loginanddownload(red_pnda_ip, username, password, remote_folder, local_destination)\n",
    "        loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "    \n",
    "    I am using it only for download full copy of /data folder from pnda VM. There is a dependency of openssh-server\n",
    "    installation on red_pnda VM.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "#         print(\"Establishing ssh connection\")\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=uname, password=pwd)\n",
    "    except paramiko.AuthenticationException:\n",
    "        print(\"Authentication failed, please verify your credentials: %s\")\n",
    "    except paramiko.SSHException as sshException:\n",
    "        print(\"Unable to establish SSH connection: %s\" % sshException)\n",
    "    except paramiko.BadHostKeyException as badHostKeyException:\n",
    "        print(\"Unable to verify server's host key: %s\" % badHostKeyException)\n",
    "    except Exception as e:\n",
    "        print(e.args)\n",
    "    try:\n",
    "#         print(\"Getting SCP Client\")\n",
    "        scpclient = scp.SCPClient(ssh_client.get_transport())\n",
    "#         print(\"Hostname: %s\", hostname)\n",
    "#         print(\"source file: %s\", sfile)\n",
    "#         print(\"target file: %s\", tfile)\n",
    "        scpclient.get(sfile,tfile, recursive = True)\n",
    "    except scp.SCPException as e:\n",
    "        print(\"Operation error: %s\", e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net:\n",
    "    \"\"\"Neural network class which loads already learned model based on previous traffic from network.\n",
    "        Based on that model we can compute uncertenity intervals. For more details plese refer to:\n",
    "        http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, n_steps, n_length, N, l):\n",
    "        \"\"\"Loads model, initiates variables for uncertenity intervals computation.\"\"\"\n",
    "        # load model from json\n",
    "        json_file = open('{}.json'.format(model_name), 'r')\n",
    "        self.model = json_file.read()\n",
    "        json_file.close()\n",
    "        self.model = model_from_json(self.model)\n",
    "        # load weights into new model\n",
    "        self.model.load_weights(\"{}.h5\".format(model_name))\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "        \"\"\"All below variables are defined based on already learned model.\"\"\"\n",
    "        # define probability for dropout in each layer with weight_decay \n",
    "        # and l - prior length scale, N - number of leanring samples\n",
    "        self.n_steps = n_steps # number of periods on which we based our prediction\n",
    "        self.n_length = n_length # period length in minutes\n",
    "        self.n_input = self.n_length * self.n_steps # define the total minutes to use as input\n",
    "        self.p_dense = 0.05\n",
    "        self.l = l\n",
    "        self.weight_decay = self.l**2/(self.n_input)\n",
    "        self.N = N\n",
    "\n",
    "        # computes tau for variance correction\n",
    "        self.tau = self.l**2 * (1 - self.p_dense) / (2 * self.N * self.weight_decay)\n",
    "        self.tau = self.tau**-1\n",
    "        \n",
    "        \"\"\"For uncertanity prediction we need to approximate our nonlinear neural network\n",
    "            with Gaussian Process. Training phase consisted of randomly dropping neurons from all layers\n",
    "            with propability p_dense and using L2 regularization with weight_decay.\n",
    "            Using random dropout on each layer we can mathematically say that our predictions are \n",
    "            almost normally distributed.\n",
    "            If we want to approximate any GaussianNN we need to use dropout the same way in prediction phase\n",
    "            as we used it in training phase.\n",
    "            In that case we need to slightly manipulate loaded model by enabling dopout in prediction phase.\n",
    "            See lines below.\"\"\"\n",
    "        # defines predict function from keras backend (tensorflow)\n",
    "        # self.model.layers[0].input - take input shape as as it was defined in first layer\n",
    "        # K.learning_phase() - if == 1 (activate droupout from learning phase)\n",
    "        # self.model.layers[-1].output - produce output vector as it was defined (shape)\n",
    "        self.f = K.function([self.model.layers[0].input, K.learning_phase()], [self.model.layers[-1].output])\n",
    "        \n",
    "        # load initial data scaler\n",
    "        if 'second' in model_name:\n",
    "            with open('/home/amadeusz/Documents/SDN_with_Neural_Network/scaler_sec_final.pkl', 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "        else:\n",
    "            with open('/home/amadeusz/Documents/SDN_with_Neural_Network/scaler_final_diff_l.pkl', 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "        \n",
    "        self.predictions = list()\n",
    "        self.uncertenity = list()\n",
    "        self.pred = list()\n",
    "        self.unc = list()\n",
    "        self.unc_up = list()\n",
    "        self.unc_down = list()\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, n_iter=10):\n",
    "        result = np.zeros((n_iter,) + (x.shape[3],))\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            result[i] = self.f([x, 1])[0].reshape(self.n_length)\n",
    "\n",
    "        prediction = result.mean(axis=0)\n",
    "        uncertainty = result.var(axis=0)\n",
    "#         print(uncertainty)\n",
    "#         uncertainty += self.tau\n",
    "        return prediction, uncertainty\n",
    "    \n",
    "    \n",
    "    def forecast(self, history):\n",
    "        # standardize input data\n",
    "        standardized = self.scaler.transform(history)\n",
    "        # flatten data\n",
    "        data = np.array(standardized)\n",
    "        data = data.reshape((data.shape[0], 1))\n",
    "        # retrieve last observations for input data\n",
    "        input_x = data[-self.n_input:, 0]\n",
    "        # reshape into [samples, time steps, rows, cols, channels]\n",
    "        input_x = input_x.reshape((1, self.n_steps, 1, self.n_length, 1))\n",
    "        yhat, uncer = self.predict_with_uncertainty(input_x)\n",
    "        return yhat, uncer\n",
    "    \n",
    "    def make_forecast(self, history):\n",
    "        # reset predictions\n",
    "        self.pred = list()\n",
    "        self.unc = list()\n",
    "        self.predictions = list()\n",
    "        self.uncertenity = list()\n",
    "#         history = [x for x in train]\n",
    "        # predict one period\n",
    "        yhat_sequence, uncert = self.forecast(history)\n",
    "        # store the predictions\n",
    "        self.predictions.append(yhat_sequence)\n",
    "        self.uncertenity.append(uncert)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.uncertenity = np.array(self.uncertenity)\n",
    "        # translate into lists\n",
    "#         test_ = list()\n",
    "        for i in self.predictions.reshape(self.predictions.shape[0]*self.predictions.shape[1]).tolist():\n",
    "            self.pred.append(i)\n",
    "        for i in self.uncertenity.reshape(self.uncertenity.shape[0]*self.uncertenity.shape[1]).tolist():\n",
    "            self.unc.append(i)\n",
    "            \n",
    "        self.unc = np.sqrt(self.scaler.inverse_transform(np.array(self.unc))) # sigma\n",
    "        \n",
    "        \n",
    "#         self.unc_up = self.scaler.inverse_transform(np.array(self.pred) + np.array(self.unc))\n",
    "#         self.unc_down = self.scaler.inverse_transform(np.array(self.pred) - np.array(self.unc))\n",
    "        \n",
    "#         self.unc_up += self.tau\n",
    "#         self.unc_down -= self.tau\n",
    "        \n",
    "        self.pred = self.scaler.inverse_transform(self.pred)\n",
    "#         self.unc = self.scaler.inverse_transform(self.unc)\n",
    "        \n",
    "#         return self.pred, self.unc_up, self.unc_down\n",
    "        return self.pred, self.unc, self.tau\n",
    "#         for i in test.reshape(test.shape[0]*test.shape[1]).tolist():\n",
    "#             test_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \"\"\"\n",
    "    Prediction class - class for preprocessing data from red_pnda.\n",
    "    Not full variables are in use (this is a changed copy of Lecturer shared file)\n",
    "    \n",
    "    Prediction class takes exacly one argument - bytes (network traffic data)\n",
    "    It is further processed and returned in other format.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, bytes_, dates, network_min, network_sec):\n",
    "        self.bytes = bytes_\n",
    "        self.dates = dates\n",
    "        self.network_min = network_min\n",
    "        self.network_sec = network_sec\n",
    "#         self.predictions_min = list()\n",
    "#         self.uncertenity_up_min = list()\n",
    "#         self.uncertenity_down_min = list()\n",
    "#         self.predictions_sec = list()\n",
    "#         self.uncertenity_up_sec = list()\n",
    "#         self.uncertenity_down_sec = list()\n",
    "        self.predictions_min = list()\n",
    "        self.sigma_min = list()\n",
    "        self.tau_min = None\n",
    "        self.predictions_sec = list()\n",
    "        self.sigma_sec = list()\n",
    "        self.tau_sec = None\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def prepare_data_for_prediction(self, interval = 'minute'):\n",
    "        \"\"\"\n",
    "        From RAW data compute time dependency (x) and bandwidth (y).\n",
    "        Find mean traffic value for each minute.\n",
    "        \"\"\"\n",
    "        self.data = self.bytes\n",
    "        self.data = pd.DataFrame(data=self.data, columns=['traffic'])\n",
    "        self.data = self.data.set_index(pd.DatetimeIndex(self.dates))\n",
    "#         print(self.train.head())\n",
    "#         print('Shape: {}'.format(self.train.shape))\n",
    "        if interval == 'minute':\n",
    "            self.data = self.data.groupby(pd.Grouper(freq='Min')).mean()\n",
    "        elif interval == 'second':\n",
    "            pass\n",
    "#         print(self.train.head())\n",
    "#         print('Shape: {}'.format(self.train.shape))\n",
    "\n",
    "    def proceed_prediction(self, interval = 'min'):\n",
    "        \"\"\"\n",
    "        Start data preprocessing.\n",
    "        \"\"\"\n",
    "        if interval == 'minute':\n",
    "            self.prepare_data_for_prediction('minute')\n",
    "#             self.predictions_min, self.uncertenity_up_min, self.uncertenity_down_min = self.network_min.make_forecast([x for x in self.data.values])\n",
    "            self.predictions_min, self.sigma_min, self.tau_min = self.network_min.make_forecast([x for x in self.data.values])\n",
    "\n",
    "        elif interval == 'second':\n",
    "            self.prepare_data_for_prediction('second')\n",
    "#             self.predictions_sec, self.uncertenity_up_sec, self.uncertenity_down_sec = self.network_sec.make_forecast([x for x in self.data.values])\n",
    "            self.predictions_sec, self.sigma_sec, self.tau_sec = self.network_sec.make_forecast([x for x in self.data.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatchDog:\n",
    "    \"\"\"\n",
    "    WatchDog class - extracts bandwidth information from collected flow and port data.\n",
    "    (Not all variables are in use)\n",
    "    \n",
    "    self.dpid - particular switch identification number\n",
    "    self.port_no - particular port number on the switch\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):    \n",
    "        self.interval_pred_min = 29*60\n",
    "        self.interval_pred_sec = 40\n",
    "        self.interval_anomaly_check_min = 60\n",
    "        self.interval_anomaly_check_sec = 1\n",
    "        self.network_min = None\n",
    "        self.network_sec = None\n",
    "        self.unusual_sec_3sig = [0, False]\n",
    "        self.unusual_min_3sig = [0, False]\n",
    "        self.unusual_sec_sig = [0, False]\n",
    "        self.unusual_min_sig = [0, False]\n",
    "        self.normal_work = True\n",
    "        self.train_data = None\n",
    "        self.pred = None\n",
    "        self.pi = None\n",
    "        self.current_stats = None\n",
    "        self.dpid = 2\n",
    "        self.port_no = 2\n",
    "        self.resampled = None\n",
    "        self.data_path = data_path\n",
    "        self.sched = BackgroundScheduler()\n",
    "        self.sched.start()\n",
    "\n",
    "        \n",
    "    def get_last_hour_stats(self):\n",
    "        \"\"\"\n",
    "        Currently not in use.\n",
    "        \"\"\"\n",
    "        last_hour_time = datetime.now() - timedelta(hours = 1)\n",
    "        year = int(last_hour_time.strftime(\"%Y\"))\n",
    "        month = int(last_hour_time.strftime(\"%m\"))\n",
    "        day = int(last_hour_time.strftime(\"%d\"))\n",
    "        hour = int(last_hour_time.strftime(\"%H\"))\n",
    "        self.train_data = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                             \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "        \n",
    "    def get_previous_stats(self):\n",
    "        \"\"\"\n",
    "        Gather statistics from particular hour/day.\n",
    "        You can change dates based on your gathered data.\n",
    "        \"\"\"\n",
    "#         year=\"2019\"\n",
    "#         month=\"1\"\n",
    "#         day=\"4\"\n",
    "#         hour=0\n",
    "        year = int(time.strftime(\"%Y\"))\n",
    "        month = int(time.strftime(\"%m\"))\n",
    "        day = int(time.strftime(\"%d\"))# - 5\n",
    "        hour = int(time.strftime(\"%H\"))\n",
    "        roznica = 0\n",
    "        d = list()\n",
    "        if hour - 5 < 0: # jezeli 5 godzin temu to byl inny dzien\n",
    "            day -= 1 # zmien dzien na wczoraj\n",
    "            roznica = np.abs(hour - 5) # policz roznice w godzinach\n",
    "            hour = 24 + (hour - 5) # policz godzine jaka byla 5 godzin temu, skaluj do wczorajszej 24-ki\n",
    "            # pobierz dane z konkretnej godziny wczoraj\n",
    "            data = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                            \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "            for i in range(1,roznica): # pobieraj dane do konca wczorajszego dnia\n",
    "                h = hour + i\n",
    "                d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                                  \"/day=\"+str(day)+\"/hour=\"+str(h)+\"/dump.json\"))\n",
    "#                 print('h = {}'.format(h))\n",
    "\n",
    "            for i in range(int(time.strftime(\"%H\"))+1): # pobieraj dane od polnocy do biezacej godziny\n",
    "                h = i\n",
    "                d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                                  \"/day=\"+str(day+1)+\"/hour=\"+str(h)+\"/dump.json\"))\n",
    "#                 print('h = {}'.format(h))\n",
    "        else: # jezeli 5 godzin temu nadal bylo dzisiaj\n",
    "            hour -= 5 # cofnij sie o 5 godzin i pobierz dane\n",
    "            data = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                            \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "            for i in range(hour+1, int(time.strftime(\"%H\"))+1): # pobieraj dane do biezacej godziny\n",
    "                h = i\n",
    "                d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                                  \"/day=\"+str(day)+\"/hour=\"+str(h)+\"/dump.json\"))\n",
    "#             print(d[1])\n",
    "\n",
    "        # zcal wszystkie pobrane dane z 5-6 godzin\n",
    "        for i in range(0,len(d)):\n",
    "            data = data.unionAll(d[i])            \n",
    "        self.train_data = data\n",
    "        \n",
    "        return hour\n",
    "    \n",
    "    def get_current_stats(self):\n",
    "        \"\"\"\n",
    "        Currently not in use.\n",
    "        \"\"\"\n",
    "        year = int(time.strftime(\"%Y\"))\n",
    "        month = int(time.strftime(\"%m\"))\n",
    "        day = int(time.strftime(\"%d\"))# - 5 # do usuniecia 2 !!!!!!!!!!!!!!!!!!!!!!!\n",
    "        hour = int(time.strftime(\"%H\"))\n",
    "        loginanddownload('192.168.57.3', 'pnda', 'pnda', '/data/year={}/month={}/day={}/hour={}/'.format(year, \n",
    "                                                                                                         month, \n",
    "                                                                                                         day,\n",
    "                                                                                                         hour),\n",
    "                         '/home/amadeusz/data/year={}/month={}/day={}/'.format(year, \n",
    "                                                                               month, \n",
    "                                                                               day, \n",
    "                                                                               hour))\n",
    "        \n",
    "        self.current_stats = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                             \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "#         self.current_stats = sqlContext.read.json(\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "#                                              \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "    \n",
    "    def get_port_stats(self,data,dpid,port_no):\n",
    "        \"\"\"\n",
    "        Gathers data from specific port on the switch.\n",
    "        \"\"\"\n",
    "        port = data.filter((data['origin']=='port_stats') & \n",
    "                           (data['switch_id']==dpid) & \n",
    "                           (data['port_no']==port_no)).orderBy('timestamp')\n",
    "        port = port.toPandas()\n",
    "        ts = pd.Series(port['timestamp'].astype(int))\n",
    "        ts = pd.to_datetime(ts, unit='s')\n",
    "        index = pd.DatetimeIndex(ts)\n",
    "        raw_data = pd.Series(port['tx_bytes'].values, index=index)\n",
    "        return raw_data, port\n",
    "    \n",
    "    def get_last_tput(self, dpid, port_no, interval):\n",
    "        \"\"\"\n",
    "        Pobieram minute ostatniego ruchu do analizy\n",
    "        \"\"\"\n",
    "        if interval == 'minute':\n",
    "            # pobieram 13 ostatnich probek (jedna probka co 5 secund czyli chce miec 60 sekund ruchu)\n",
    "            last_minute = self.current_stats.filter((self.current_stats['origin']=='port_stats') & \n",
    "                               (self.current_stats['switch_id']==dpid) & \n",
    "                               (self.current_stats['port_no']==port_no)).orderBy('timestamp', ascending=False).limit(13)\n",
    "            last_minute = last_minute.toPandas()\n",
    "            bytes_ = last_minute['tx_bytes'].astype(int)\n",
    "            time_ = last_minute['timestamp'].astype(int)\n",
    "            tput = (bytes_[0]-bytes_.iloc[-1])/(time_[0]-time_.iloc[-1]) # licze roznice w ruchu jaki byl 60s temu i teraz i dziele przez czas\n",
    "        elif interval == 'second':\n",
    "            # pobieram 2 ostatnie probki (jedna probka co 5 secund czyli chce miec 5 sekund ruchu, wyciagam srednia na sekunde)\n",
    "            last_minute = self.current_stats.filter((self.current_stats['origin']=='port_stats') & \n",
    "                               (self.current_stats['switch_id']==dpid) & \n",
    "                               (self.current_stats['port_no']==port_no)).orderBy('timestamp', ascending=False).limit(2)\n",
    "            last_minute = last_minute.toPandas()\n",
    "            bytes_ = last_minute['tx_bytes'].astype(int)\n",
    "            time_ = last_minute['timestamp'].astype(int)\n",
    "            tput = (bytes_[0]-bytes_.iloc[-1])/(time_[0]-time_.iloc[-1]) # licze roznice w ruchu jaki byl w czasie 2s\n",
    "       \n",
    "        return tput*8/1e6 #Mbps (licze/zamieniam na Mbps)\n",
    "        \n",
    "    def resample_port_stats (self,raw_data, port, start):\n",
    "        \"\"\"\n",
    "        Resamples data into correct date format and frequency.\n",
    "        \"\"\"\n",
    "        raw_data = raw_data[~raw_data.index.duplicated(keep='first')]\n",
    "        resampled_data = raw_data.resample('s').interpolate()\n",
    "        resampled_data = [(y - x) for x,y in zip(resampled_data.values,resampled_data.values[1:])]\n",
    "        ts_resampled = pd.Series(range(len(resampled_data))) + start*60*60\n",
    "        ts_resampled= pd.to_datetime(ts_resampled, unit='s')\n",
    "        return resampled_data, ts_resampled\n",
    "    \n",
    "    \n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    def check_current_traffic(self, mode):\n",
    "#         loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "        hour = int(time.strftime(\"%H\"))\n",
    "        minute = int(time.strftime(\"%M\"))\n",
    "        second = int(time.strftime(\"%S\"))\n",
    "        if mode == 'minute':\n",
    "\n",
    "            # upper and lower limits for minute prediction\n",
    "#             up_min = self.pred_check_data_min.loc[(self.pred_check_data_min['hour'] == hour) & \n",
    "#                                                   (self.pred_check_data_min['minute'] == minute)]['up'].values[0]\n",
    "#             down_min = self.pred_check_data_min.loc[(self.pred_check_data_min['hour'] == hour) & \n",
    "#                                                     (self.pred_check_data_min['minute'] == minute)]['down'].values[0]\n",
    "            \n",
    "            mean_min, sigma_min, tau_min = self.pred_check_data_min.loc[(self.pred_check_data_min['hour'] == hour) & \n",
    "                                                                        (self.pred_check_data_min['minute'] == minute)][['mean','sigma','tau']].values[0].tolist()\n",
    "            \n",
    "            \n",
    "#             print('up: {}'.format(up_min))\n",
    "#             print('down: {}'.format(down_min))\n",
    "            self.get_current_stats() # pobierz dane z teraz\n",
    "            cur_tput_min = self.get_last_tput(self.dpid, self.port_no, 'minute') # wyodrebnij tylko dane z danego portu na switchu\n",
    "            _ = system('clear') \n",
    "            print('tput min = {}'.format(cur_tput_min))\n",
    "#             if cur_tput_min > up_min or cur_tput_min < down_min:\n",
    "#                 print('Anomaly detected!!! (minutes interval)')\n",
    "#                 self.normal_work = False\n",
    "#             else:\n",
    "#                 print('Normal traffic... (minutes interval)')\n",
    "#                 self.normal_work = True\n",
    "            # mean+4sigma > cur_tput_min > mean-4sigma > 0\n",
    "            if (cur_tput_min > (mean_min + (sigma_min + tau_min)*4)) or (cur_tput_min < (mean_min - (sigma_min + tau_min)*4)) or (cur_tput_min == 0):\n",
    "                print('Anomaly detected!!! (4 sigma minutes)')\n",
    "                self.normal_work = False\n",
    "            elif not self.normal_work:\n",
    "                print('Returning to normal state. (4 sigma minutes)')\n",
    "                self.normal_work = True\n",
    "            # mean+3sigma > cur_tput_min > mean-3sigma\n",
    "            if (cur_tput_min > (mean_min + (sigma_min + tau_min)*3)) or (cur_tput_min < (mean_min - (sigma_min + tau_min)*3)):\n",
    "                if not self.unusual_min_3sig[1]: # if previously it was not an anomaly\n",
    "                    self.unusual_min_3sig[0] += 1\n",
    "                    self.unusual_min_3sig[1] = True\n",
    "                else:\n",
    "                    self.unusual_min_3sig[0] += 1\n",
    "                    if self.unusual_min_3sig[0] == 4:\n",
    "                        print('Anomaly detected!!! (3 sigma interval minutes)')\n",
    "                        self.normal_work = False\n",
    "            else:\n",
    "                if self.unusual_min_3sig[1]:\n",
    "                    self.unusual_min_3sig[0] = 0\n",
    "                    self.unusual_min_3sig[1] = False\n",
    "                    print('Returning to normal state. (3 sigma minutes)')\n",
    "                    self.normal_work = True\n",
    "            # mean+sigma > cur_tput_min > mean-sigma\n",
    "            if (cur_tput_min > (mean_min + (sigma_min + tau_min))) or (cur_tput_min < (mean_min - (sigma_min + tau_min))):\n",
    "                if not self.unusual_min_sig[1]: # if previously it was not an anomaly\n",
    "                    self.unusual_min_sig[0] += 1\n",
    "                    self.unusual_min_sig[1] = True\n",
    "                else:\n",
    "                    self.unusual_min_sig[0] += 1\n",
    "                    if self.unusual_min_sig[0] == 50:\n",
    "                        print('Anomaly detected!!! (sigma interval minutes)')\n",
    "                        self.normal_work = False\n",
    "            else:\n",
    "                if self.unusual_min_sig[1]:\n",
    "                    self.unusual_min_sig[0] = 0\n",
    "                    self.unusual_min_sig[1] = False\n",
    "                    print('Returning to normal state. (sigma minutes)')\n",
    "                    self.normal_work = True\n",
    "                \n",
    "                \n",
    "        elif mode == 'second':\n",
    "            # upper and lower limits for second prediction\n",
    "#             up_sec = self.pred_check_data_sec.loc[(self.pred_check_data_sec['hour'] == hour) & \n",
    "#                                                   (self.pred_check_data_sec['minute'] == minute) &\n",
    "#                                                   (pred_check_data_sec['second'] == second)]['up'].values[0]\n",
    "#             down_sec = self.pred_check_data_sec.loc[(self.pred_check_data_sec['hour'] == hour) & \n",
    "#                                                     (self.pred_check_data_sec['minute'] == minute) &\n",
    "#                                                     (pred_check_data_sec['second'] == second)]['down'].values[0]\n",
    "            \n",
    "            mean_sec, sigma_sec, tau_sec = self.pred_check_data_sec.loc[(self.pred_check_data_sec['hour'] == hour) & \n",
    "                                                                        (self.pred_check_data_sec['minute'] == minute) &\n",
    "                                                                        (self.pred_check_data_sec['second'] == second)][['mean','sigma','tau']].values[0].tolist()\n",
    "            \n",
    "    \n",
    "#             print('up: {}'.format(up_sec))\n",
    "#             print('down: {}'.format(down_sec))\n",
    "            self.get_current_stats() # pobierz dane z teraz\n",
    "            cur_tput_sec = self.get_last_tput(self.dpid, self.port_no, 'second') # wyodrebnij tylko dane z danego portu na switchu\n",
    "            _ = system('clear') \n",
    "            print('tput sec = {}'.format(cur_tput_sec))\n",
    "#             if (cur_tput_sec > up_sec or cur_tput_sec < down_sec):\n",
    "#                 self.unusual += 1\n",
    "#                 if self.unusual >= 50:\n",
    "#                     print('Anomaly detected!!! (seconds interval)')\n",
    "#                     self.normal_work = False\n",
    "#             else:\n",
    "#                 self.unusual = 0\n",
    "#                 print('Normal traffic... (seconds interval)')\n",
    "#                 self.normal_work = True\n",
    "\n",
    "            # mean+4sigma > cur_tput_sec > mean-4sigma > 0\n",
    "            if (cur_tput_sec > (mean_sec + (sigma_sec + tau_sec)*4)) or (cur_tput_sec < (mean_sec - (sigma_sec + tau_sec)*4)) or (cur_tput_sec == 0):\n",
    "                print('Anomaly detected!!! (4 sigma seconds)')\n",
    "                self.normal_work = False\n",
    "            elif not self.normal_work:\n",
    "                print('Returning to normal state. (4 sigma seconds)')\n",
    "                self.normal_work = True\n",
    "            # mean+3sigma > cur_tput_sec > mean-3sigma\n",
    "            if (cur_tput_sec > (mean_sec + (sigma_sec + tau_sec)*3)) or (cur_tput_sec < (mean_sec - (sigma_sec + tau_sec)*3)):\n",
    "                if not self.unusual_sec_3sig[1]: # if previously it was not an anomaly\n",
    "                    self.unusual_sec_3sig[0] += 1\n",
    "                    self.unusual_sec_3sig[1] = True\n",
    "                else:\n",
    "                    self.unusual_sec_3sig[0] += 1\n",
    "                    if self.unusual_sec_3sig[0] == 4:\n",
    "                        print('Anomaly detected!!! (3 sigma interval seconds)')\n",
    "                        self.normal_work = False\n",
    "            else:\n",
    "                if self.unusual_sec_3sig[1]:\n",
    "                    self.unusual_sec_3sig[0] = 0\n",
    "                    self.unusual_sec_3sig[1] = False\n",
    "                    print('Returning to normal state. (3 sigma seconds)')\n",
    "                    self.normal_work = True\n",
    "            # mean+sigma > cur_tput_min > mean-sigma\n",
    "            if (cur_tput_sec > (mean_sec + (sigma_sec + tau_sec))) or (cur_tput_sec < (mean_sec - (sigma_sec + tau_sec))):\n",
    "                if not self.unusual_sec_sig[1]: # if previously it was not an anomaly\n",
    "                    self.unusual_sec_sig[0] += 1\n",
    "                    self.unusual_sec_sig[1] = True\n",
    "                else:\n",
    "                    self.unusual_sec_sig[0] += 1\n",
    "                    if self.unusual_sec_sig[0] == 50:\n",
    "                        print('Anomaly detected!!! (sigma interval seconds)')\n",
    "                        self.normal_work = False\n",
    "            else:\n",
    "                if self.unusual_sec_sig[1]:\n",
    "                    self.unusual_sec_sig[0] = 0\n",
    "                    self.unusual_sec_sig[1] = False\n",
    "                    print('Returning to normal state. (sigma seconds)')\n",
    "                    self.normal_work = True\n",
    "        \n",
    "            \n",
    "#         if self.unusual > 2 and self.normal_work == True:\n",
    "#             self.change_interval(5)\n",
    "#             self.normal_work = False\n",
    "#             print('changing interval to 5s')\n",
    "#         elif self.unusual == 0 and self.normal_work == False:\n",
    "#             self.change_interval(300)\n",
    "#             self.normal_work = True\n",
    "#             print('changing interval to 300s')\n",
    "            \n",
    "    # wystartuj observer i monitoruj/zmieniaj interval ??\n",
    "    def start_stats_observer(self):\n",
    "        self.sched.add_job(self.proceed_prediction, 'interval', seconds=self.interval_pred_min, kwargs = {'enable_plot': True, 'mode': 'minute', 'plot_prediction': True})\n",
    "        self.sched.add_job(self.proceed_prediction, 'interval', seconds=self.interval_pred_sec, kwargs = {'enable_plot': True, 'mode': 'second', 'plot_prediction': True})\n",
    "#         self.sched.add_job(self.check_current_traffic, 'interval', seconds=self.interval_anomaly_check_sec, args = ['second'])\n",
    "#         self.sched.add_job(self.check_current_traffic, 'interval', seconds=self.interval_anomaly_check_min, args = ['minute'])\n",
    "\n",
    "#         self.interval_pred_min = 29*60\n",
    "#         self.interval_pred_sec = 40\n",
    "#         self.interval_anomaly_check_min = 60\n",
    "#         self.interval_anomaly_check_sec = 1\n",
    "        \n",
    "    def change_job_interval(self,interval):\n",
    "        print(\"Rescheduling stat request to %i seconds\", interval)\n",
    "        for s in self.sched.get_jobs():\n",
    "            print('rescheduling job %s', s.id)\n",
    "            it = IntervalTrigger(seconds=interval)\n",
    "            self.sched.reschedule_job(s.id, trigger=it)\n",
    "\n",
    "    def change_interval(self, interval):\n",
    "        print(\"changing interval to %i\" % interval)\n",
    "        self.send_stats_interval(interval)\n",
    "        self.change_job_interval(interval)\n",
    "        \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def proceed_prediction(self, enable_plot, mode, plot_prediction):\n",
    "        \"\"\"\n",
    "        Start preprocessing traffic data.\n",
    "        \n",
    "        If you want to download data to your localhost just uncomment first line.\n",
    "        This is not necessary when you have all the data stored locally already.\n",
    "        \"\"\"\n",
    "#         loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "        year = int(time.strftime(\"%Y\"))\n",
    "        month = int(time.strftime(\"%m\"))\n",
    "        day = int(time.strftime(\"%d\"))\n",
    "        loginanddownload('192.168.57.3', 'pnda', 'pnda', '/data/year={}/month={}/day={}/'.format(year, month, day),\n",
    "                         '/home/amadeusz/data/year={}/month={}/'.format(year, month, day))\n",
    "        self.get_current_stats()\n",
    "        if self.network_min is None and self.network_sec is None:\n",
    "            self.network_min = Neural_Net('/home/amadeusz/Documents/SDN_with_Neural_Network/model_standardise_with_l_higher_wd_bayasian_RNN', 10, 30, 1440*6, 1e-1) # loads neural network model minutes\n",
    "            self.network_sec = Neural_Net('/home/amadeusz/Documents/SDN_with_Neural_Network/model_standardise_seconds_l_bayasian_RNN', 10, 60, 86400, 1e-1) # loads neural network model seconds\n",
    "        \n",
    "        start = self.get_previous_stats() # pobierz 5 godzin ruchu od teraz wstecz\n",
    "        data, port_stats = self.get_port_stats(self.train_data, self.dpid, self.port_no) # wydziel ruch z danego portu\n",
    "        resampled_data, ts_resampled = self.resample_port_stats(data, port_stats, start) # przygotuj dane\n",
    "        \n",
    "        # test (jezeli robie bez wlaczonej kafki, musze usunac dopelnienie do godziny ruchu bo mam juz pobrany)\n",
    "#         minute = 60 - int(time.strftime(\"%M\"))\n",
    "#         resampled_data = resampled_data[:-minute*60]\n",
    "#         ts_resampled = ts_resampled[:-minute*60]\n",
    "    \n",
    "        self.ts_resampled = ts_resampled # store time vector\n",
    "        self.resampled = [x*8/1e6 for x in resampled_data] # zamien na Mbps (nadal w przedzialach sekundowych)\n",
    "\n",
    "        prediction = Prediction(self.resampled, ts_resampled, self.network_min, self.network_sec) # prepare data for prediction\n",
    "        if mode == 'minute':\n",
    "            prediction.proceed_prediction('minute') # make 30 minutes prediction\n",
    "        elif mode == 'second':\n",
    "            prediction.proceed_prediction('second') # make 60 seconds prediction\n",
    "        \n",
    "        # plotting part (plots 5 hours back and predicted mean with uncertenity)\n",
    "        if enable_plot:\n",
    "            if mode == 'minute':\n",
    "#                 fig = plt.figure(figsize=(15,8))\n",
    "#                 ax = fig.add_subplot(111)\n",
    "                # resample 5 hour data from seconds into minutes\n",
    "                ixy = pd.DataFrame(data=[x*8/1e6 for x in resampled_data], columns=['traffic'])\n",
    "                ixy = ixy.set_index(pd.DatetimeIndex(ts_resampled))\n",
    "                ixy = ixy.groupby(pd.Grouper(freq='Min')).mean()\n",
    "                # \n",
    "                new_pred = prediction.predictions_min.tolist()\n",
    "#                 new_unc_up = prediction.uncertenity_up_min.tolist()\n",
    "#                 new_unc_down = prediction.uncertenity_down_min.tolist()\n",
    "                new_sigma_min = prediction.sigma_min.tolist()\n",
    "                for position, i in enumerate(ixy.values.tolist()):\n",
    "                    new_pred.insert(position,i[0])\n",
    "#                     new_unc_up.insert(position,i[0])\n",
    "#                     new_unc_down.insert(position,i[0])\n",
    "                    new_sigma_min.insert(position,i[0])\n",
    "                # creates datetime range for plots\n",
    "                if self.ts_resampled.iloc[-1].minute + 30 < 60:\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[0].year,\n",
    "                                                                       self.ts_resampled.iloc[0].month,\n",
    "                                                                       self.ts_resampled.iloc[0].day,\n",
    "                                                                       self.ts_resampled.iloc[0].hour,\n",
    "                                                                       self.ts_resampled.iloc[0].minute), \n",
    "                                      end='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     self.ts_resampled.iloc[-1].day,\n",
    "                                                                     self.ts_resampled.iloc[-1].hour,\n",
    "                                                                     self.ts_resampled.iloc[-1].minute+30), \n",
    "                                      freq=\"1min\")\n",
    "                else:\n",
    "                    print('jednak tutaj weszlo')\n",
    "                    if self.ts_resampled.iloc[-1].hour+1 >= 24:\n",
    "                        hour = 0\n",
    "                        day = self.ts_resampled.iloc[-1].day + 1\n",
    "                    else:\n",
    "                        hour = self.ts_resampled.iloc[-1].hour+1\n",
    "                        day = self.ts_resampled.iloc[-1].day\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[0].year,\n",
    "                                                                       self.ts_resampled.iloc[0].month,\n",
    "                                                                       self.ts_resampled.iloc[0].day,\n",
    "                                                                       self.ts_resampled.iloc[0].hour,\n",
    "                                                                       self.ts_resampled.iloc[0].minute), \n",
    "                                      end='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     day,\n",
    "                                                                     hour,\n",
    "                                                                     (self.ts_resampled.iloc[-1].minute+30) - 60), \n",
    "                                      freq=\"1min\")\n",
    "                x2 = pd.date_range(start='{}'.format(self.ts_resampled.iloc[0]), \n",
    "                                      end='{}'.format(self.ts_resampled.iloc[-1]), \n",
    "                                      freq=\"1min\")\n",
    "\n",
    "                self.pred_check_data_min = pd.DataFrame(data=np.full(len(new_sigma_min), prediction.tau_min), \n",
    "                                                    columns=['tau'])\n",
    "\n",
    "#                 self.pred_check_data_min['up'] = new_unc_up\n",
    "#                 self.pred_check_data_min['down'] = new_unc_down\n",
    "                self.pred_check_data_min['sigma'] = new_sigma_min\n",
    "                self.pred_check_data_min['mean'] = new_pred\n",
    "                self.pred_check_data_min['hour'] = [p.hour for p in pd.to_datetime(x1)]\n",
    "                self.pred_check_data_min['minute'] = [p.minute for p in pd.to_datetime(x1)]\n",
    "                \n",
    "                if plot_prediction:\n",
    "#                     ax.plot(x1, new_pred, 'g') # plot prediction mean\n",
    "#                     ax.fill_between(x1, new_unc_up, new_unc_down, facecolor='yellow') # plot prediction uncertenity interval\n",
    "                    \n",
    "                    new_pred = prediction.predictions_min.tolist()\n",
    "                    new_sigma_min = prediction.sigma_min.tolist()\n",
    "                    new_4sigma_min_up = np.asarray(new_pred) + (np.asarray(new_sigma_min) + prediction.tau_min)*4\n",
    "                    new_4sigma_min_up = new_4sigma_min_up.tolist()\n",
    "                    new_4sigma_min_down = np.asarray(new_pred) - (np.asarray(new_sigma_min) + prediction.tau_min)*4\n",
    "                    new_4sigma_min_down = new_4sigma_min_down.tolist()\n",
    "                    new_3sigma_min_up = np.asarray(new_pred) + (np.asarray(new_sigma_min) + prediction.tau_min)*3\n",
    "                    new_3sigma_min_up = new_3sigma_min_up.tolist()\n",
    "                    new_3sigma_min_down = np.asarray(new_pred) - (np.asarray(new_sigma_min) + prediction.tau_min)*3\n",
    "                    new_3sigma_min_down = new_3sigma_min_down.tolist()\n",
    "                    new_sigma_min_up = np.asarray(new_pred) + (np.asarray(new_sigma_min) + prediction.tau_min)\n",
    "                    new_sigma_min_up = new_sigma_min_up.tolist()\n",
    "                    new_sigma_min_down = np.asarray(new_pred) - (np.asarray(new_sigma_min) + prediction.tau_min)\n",
    "                    new_sigma_min_down = new_sigma_min_down.tolist()\n",
    "                    \n",
    "                    fig = plt.figure(figsize=(15,8))\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    ax.plot(x1[-len(new_pred):], new_pred, 'g', label = 'predicted mean') # plot prediction mean\n",
    "                    # 4*sigma\n",
    "                    ax.fill_between(x1[-len(new_pred):], \n",
    "                                    new_4sigma_min_up, \n",
    "                                    new_4sigma_min_down, \n",
    "                                    facecolor='red', \n",
    "                                    alpha=0.2,\n",
    "                                    label = '4 sigma')\n",
    "                    # 3*sigma\n",
    "                    ax.fill_between(x1[-len(new_pred):], \n",
    "                                    new_3sigma_min_up, \n",
    "                                    new_3sigma_min_down, \n",
    "                                    facecolor='red', \n",
    "                                    alpha=0.4,\n",
    "                                    label = '3 sigma')\n",
    "                    # sigma\n",
    "                    ax.fill_between(x1[-len(new_pred):], \n",
    "                                    new_sigma_min_up, \n",
    "                                    new_sigma_min_down, \n",
    "                                    facecolor='red', \n",
    "                                    alpha=0.7,\n",
    "                                    label = 'sigma')\n",
    "                    ax.legend()\n",
    "                    ax.set_title('Predicted 30 minutes with uncertenity intervals.')\n",
    "                    ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "                    ax.set_xlabel('hour:min')\n",
    "                    ax.set_ylim(bottom = 0)\n",
    "                    \n",
    "                    for position, i in enumerate(ixy.values.tolist()):\n",
    "                        new_pred.insert(position,i[0])\n",
    "                        new_4sigma_min_up.insert(position,i[0])\n",
    "                        new_4sigma_min_down.insert(position,i[0])\n",
    "                        new_3sigma_min_up.insert(position,i[0])\n",
    "                        new_3sigma_min_down.insert(position,i[0])\n",
    "                        new_sigma_min_up.insert(position,i[0])\n",
    "                        new_sigma_min_down.insert(position,i[0])\n",
    "                        \n",
    "                    fig = plt.figure(figsize=(15,8))\n",
    "                    ax = fig.add_subplot(111)\n",
    "                        \n",
    "                    ax.plot(x1, new_pred, 'g', label = 'predicted mean') # plot prediction mean\n",
    "                    # 4*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_4sigma_min_up, \n",
    "                                    new_4sigma_min_down, \n",
    "                                    facecolor='red', alpha=0.2, label = '4 sigma')\n",
    "                    # 3*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_3sigma_min_up, \n",
    "                                    new_3sigma_min_down, \n",
    "                                    facecolor='red', alpha=0.4, label = '3 sigma')\n",
    "                    # sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_sigma_min_up, \n",
    "                                    new_sigma_min_down, \n",
    "                                    facecolor='red', alpha=0.7, label = 'sigma')\n",
    "                    \n",
    "                    ax.plot(x2, ixy.values, 'b', label = 'known values') # plot 5 hours back\n",
    "                    ax.legend(loc = 'upper left')\n",
    "                    ax.set_title('5 hours traffic and predicted 30 min with uncertenity intervals.')\n",
    "                    ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "                    ax.set_xlabel('hour:min')\n",
    "                    ax.set_ylim(bottom = 0)\n",
    "                else:\n",
    "                    fig = plt.figure(figsize=(15,8))\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    ax.plot(x2, ixy.values, 'b', label = 'known values') # plot 5 hours back\n",
    "                    ax.legend(loc = 'upper left')\n",
    "                    ax.set_title('5 hours traffic and predicted 30 min with uncertenity intervals.')\n",
    "                    ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "                    ax.set_xlabel('hour:min')\n",
    "                    ax.set_ylim(bottom = 0)\n",
    "                \n",
    "            elif mode == 'second':\n",
    "                # seconds part\n",
    "#                 fig = plt.figure(figsize=(15,8))\n",
    "#                 ax = fig.add_subplot(111)\n",
    "                # resample 5 hour data from seconds into minutes\n",
    "                ixy = pd.DataFrame(data=[x*8/1e6 for x in resampled_data][-600:], columns=['traffic'])\n",
    "                ixy = ixy.set_index(pd.DatetimeIndex(ts_resampled[-600:]))\n",
    "                # \n",
    "                new_pred = prediction.predictions_sec.tolist()\n",
    "#                 new_unc_up = prediction.uncertenity_up_sec.tolist()\n",
    "#                 new_unc_down = prediction.uncertenity_down_sec.tolist()\n",
    "                new_sigma_sec = prediction.sigma_sec.tolist()\n",
    "                for position, i in enumerate(ixy.values.tolist()):\n",
    "                    new_pred.insert(position,i[0])\n",
    "#                     new_unc_up.insert(position,i[0])\n",
    "#                     new_unc_down.insert(position,i[0])\n",
    "                    new_sigma_sec.insert(position,i[0])\n",
    "                # creates datetime range for plots\n",
    "                if self.ts_resampled.iloc[-1].minute + 1 < 60:\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-600].year,\n",
    "                                                                       self.ts_resampled.iloc[-600].month,\n",
    "                                                                       self.ts_resampled.iloc[-600].day,\n",
    "                                                                       self.ts_resampled.iloc[-600].hour,\n",
    "                                                                       self.ts_resampled.iloc[-600].minute,\n",
    "                                                                       self.ts_resampled.iloc[-600].second), \n",
    "                                      end='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     self.ts_resampled.iloc[-1].day,\n",
    "                                                                     self.ts_resampled.iloc[-1].hour,\n",
    "                                                                     self.ts_resampled.iloc[-1].minute+1,\n",
    "                                                                     self.ts_resampled.iloc[-1].second), \n",
    "                                      freq=\"1S\")\n",
    "                else:\n",
    "                    print('jednak tutaj weszlo')\n",
    "                    if self.ts_resampled.iloc[-1].hour+1 >= 24:\n",
    "                        hour = 0\n",
    "                        day = self.ts_resampled.iloc[-1].day + 1\n",
    "                    else:\n",
    "                        hour = self.ts_resampled.iloc[-1].hour+1\n",
    "                        day = self.ts_resampled.iloc[-1].day\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-600].year,\n",
    "                                                                       self.ts_resampled.iloc[-600].month,\n",
    "                                                                       self.ts_resampled.iloc[-600].day,\n",
    "                                                                       self.ts_resampled.iloc[-600].hour,\n",
    "                                                                       self.ts_resampled.iloc[-600].minute,\n",
    "                                                                       self.ts_resampled.iloc[-600].second), \n",
    "                                      end='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     day,\n",
    "                                                                     hour,\n",
    "                                                                     '00',\n",
    "                                                                     self.ts_resampled.iloc[-1].second), \n",
    "                                      freq=\"1S\")\n",
    "                x2 = pd.date_range(start='{}'.format(self.ts_resampled.iloc[-600]), \n",
    "                                      end='{}'.format(self.ts_resampled.iloc[-1]), \n",
    "                                      freq=\"1S\")\n",
    "\n",
    "                self.pred_check_data_sec = pd.DataFrame(data=np.full(len(new_sigma_sec), prediction.tau_sec), \n",
    "                                                    columns=['tau'])\n",
    "\n",
    "#                 self.pred_check_data_sec['up'] = new_unc_up\n",
    "#                 self.pred_check_data_sec['down'] = new_unc_down\n",
    "                self.pred_check_data_sec['sigma'] = new_sigma_sec\n",
    "                self.pred_check_data_sec['mean'] = new_pred\n",
    "                self.pred_check_data_sec['hour'] = [p.hour for p in pd.to_datetime(x1)]\n",
    "                self.pred_check_data_sec['minute'] = [p.minute for p in pd.to_datetime(x1)]\n",
    "                self.pred_check_data_sec['second'] = [p.second for p in pd.to_datetime(x1)]\n",
    "                \n",
    "                if plot_prediction:\n",
    "                    new_pred = prediction.predictions_sec.tolist()\n",
    "                    new_sigma_sec = prediction.sigma_sec.tolist()\n",
    "                    new_4sigma_sec_up = np.asarray(new_pred) + (np.asarray(new_sigma_sec) + prediction.tau_sec)*4\n",
    "                    new_4sigma_sec_up = new_4sigma_sec_up.tolist()\n",
    "                    new_4sigma_sec_down = np.asarray(new_pred) - (np.asarray(new_sigma_sec) + prediction.tau_sec)*4\n",
    "                    new_4sigma_sec_down = new_4sigma_sec_down.tolist()\n",
    "                    new_3sigma_sec_up = np.asarray(new_pred) + (np.asarray(new_sigma_sec) + prediction.tau_sec)*3\n",
    "                    new_3sigma_sec_up = new_3sigma_sec_up.tolist()\n",
    "                    new_3sigma_sec_down = np.asarray(new_pred) - (np.asarray(new_sigma_sec) + prediction.tau_sec)*3\n",
    "                    new_3sigma_sec_down = new_3sigma_sec_down.tolist()\n",
    "                    new_sigma_sec_up = np.asarray(new_pred) + (np.asarray(new_sigma_sec) + prediction.tau_sec)\n",
    "                    new_sigma_sec_up = new_sigma_sec_up.tolist()\n",
    "                    new_sigma_sec_down = np.asarray(new_pred) - (np.asarray(new_sigma_sec) + prediction.tau_sec)\n",
    "                    new_sigma_sec_down = new_sigma_sec_down.tolist()\n",
    "                    \n",
    "                    fig = plt.figure(figsize=(15,8))\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    ax.plot(x1[-len(new_pred):], new_pred, 'g', label = 'predicted mean') # plot prediction mean\n",
    "                    # 4*sigma\n",
    "                    ax.fill_between(x1[-len(new_pred):], \n",
    "                                    new_4sigma_sec_up, \n",
    "                                    new_4sigma_sec_down, \n",
    "                                    facecolor='red', \n",
    "                                    alpha=0.2,\n",
    "                                    label = '4 sigma')\n",
    "                    # 3*sigma\n",
    "                    ax.fill_between(x1[-len(new_pred):], \n",
    "                                    new_3sigma_sec_up, \n",
    "                                    new_3sigma_sec_down, \n",
    "                                    facecolor='red', \n",
    "                                    alpha=0.4,\n",
    "                                    label = '3 sigma')\n",
    "                    # sigma\n",
    "                    ax.fill_between(x1[-len(new_pred):], \n",
    "                                    new_sigma_sec_up, \n",
    "                                    new_sigma_sec_down, \n",
    "                                    facecolor='red', \n",
    "                                    alpha=0.7,\n",
    "                                    label = 'sigma')\n",
    "                    ax.legend()\n",
    "                    ax.set_title('Predicted 60 seconds with uncertenity intervals.')\n",
    "                    ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%M:%S'))\n",
    "                    ax.set_xlabel('min:sec')\n",
    "                    ax.set_ylim(bottom = 0)\n",
    "                    \n",
    "                    for position, i in enumerate(ixy.values.tolist()):\n",
    "                        new_pred.insert(position,i[0])\n",
    "                        new_4sigma_sec_up.insert(position,i[0])\n",
    "                        new_4sigma_sec_down.insert(position,i[0])\n",
    "                        new_3sigma_sec_up.insert(position,i[0])\n",
    "                        new_3sigma_sec_down.insert(position,i[0])\n",
    "                        new_sigma_sec_up.insert(position,i[0])\n",
    "                        new_sigma_sec_down.insert(position,i[0])\n",
    "                        \n",
    "                    fig = plt.figure(figsize=(15,8))\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    \n",
    "                    \n",
    "                    ax.plot(x1, new_pred, 'g', label = 'predicted mean') # plot prediction mean\n",
    "#                     ax.fill_between(x1, new_unc_up, new_unc_down, facecolor='yellow') # plot prediction uncertenity interval\n",
    "                    # 4*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_4sigma_sec_up, \n",
    "                                    new_4sigma_sec_down, \n",
    "                                    facecolor='red', alpha=0.2, label = '4 sigma')\n",
    "                    # 3*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_3sigma_sec_up, \n",
    "                                    new_3sigma_sec_down, \n",
    "                                    facecolor='red', alpha=0.4, label = '3 sigma')\n",
    "                    # sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_sigma_sec_up, \n",
    "                                    new_sigma_sec_down, \n",
    "                                    facecolor='red', alpha=0.7, label = 'sigma')\n",
    "                    ax.plot(x2, ixy.values, 'b', label = 'known values') # plot 5 hours back\n",
    "                    ax.legend(loc = 'upper left')\n",
    "                    ax.set_title('600 seconds traffic and predicted 60 sec with uncertenity intervals.')\n",
    "                    ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%M:%S'))\n",
    "                    ax.set_xlabel('min:sec')\n",
    "                    ax.set_ylim(bottom = 0)\n",
    "                else:\n",
    "                    fig = plt.figure(figsize=(15,8))\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    ax.plot(x2, ixy.values, 'b', label = 'known values') # plot 5 hours back\n",
    "                    ax.legend(loc = 'upper left')\n",
    "                    ax.set_title('600 seconds traffic and predicted 60 sec with uncertenity intervals.')\n",
    "                    ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%M:%S'))\n",
    "                    ax.set_xlabel('min:sec')\n",
    "                    ax.set_ylim(bottom = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# import matplotlib.animation as animation\n",
    "# x = np.linspace(0,100,100)\n",
    "# y = np.linspace(0,100,100)\n",
    "\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# def animate(i):\n",
    "#     y[np.random.randint(low=0,high=99)] = np.random.randint(low=0,high=99)\n",
    "#     ax.clear()\n",
    "#     ax.plot(x,y)\n",
    "#     time.sleep(5)\n",
    "\n",
    "# ani = animation.FuncAnimation(fig, animate)\n",
    "# plt.show()\n",
    "# # print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# x = np.linspace(0,100,100)\n",
    "# y = np.linspace(0,100,100)\n",
    "\n",
    "# plt.ion()\n",
    "# fig = plt.figure()\n",
    "# ax = plt.gca()\n",
    "# ax.set_autoscale_on(True)\n",
    "# line, = ax.plot(1, 1)\n",
    "\n",
    "# for i in range(6):\n",
    "#     line.set_ydata(y)\n",
    "#     line.set_xdata(x)\n",
    "#     ax.relim()\n",
    "#     ax.autoscale_view(True,True,True)\n",
    "#     plt.draw()\n",
    "#     y[np.random.randint(low=0,high=99)] = np.random.randint(low=0,high=99)\n",
    "#     plt.pause(3)\n",
    "#     print('hello')\n",
    "# #     time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # please specify path to data folder\n",
    "# %matplotlib inline\n",
    "# # %matplotlib qt\n",
    "# wd = WatchDog(\"/home/amadeusz/\")\n",
    "# start = time.time()\n",
    "# wd.proceed_prediction(enable_plot = True, mode = 'minute', plot_prediction = True)\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "# start = time.time()\n",
    "# wd.proceed_prediction(enable_plot = True, mode = 'second', plot_prediction = True)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import signal\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "class ClientThread (threading.Thread):\n",
    "\n",
    "    def __init__(self, threadID, mode):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        # The shutdown_flag is a threading.Event object that\n",
    "            # indicates whether the thread should be terminated.\n",
    "        self.shutdown_flag = threading.Event()\n",
    "\n",
    "        # ... Other thread setup code here ...\n",
    "        self.threadID = threadID\n",
    "        self.mode = mode\n",
    "        self.x = np.linspace(0,100,100)\n",
    "        self.y = np.linspace(0,100,100)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print('Thread #%s started' % self.ident)\n",
    "        while not self.shutdown_flag.is_set():\n",
    "            if self.mode == 'predict_min':\n",
    "                wd.proceed_prediction(enable_plot = True, mode = 'minute', plot_prediction = False)\n",
    "                time.sleep(29*60)\n",
    "            elif self.mode == 'predict_sec':\n",
    "                wd.proceed_prediction(enable_plot = True, mode = 'second', plot_prediction = False)\n",
    "                time.sleep(40)\n",
    "            elif self.mode == 'check_min':\n",
    "                wd.check_current_traffic('minute')\n",
    "                time.sleep(60)\n",
    "            elif self.mode == 'check_sec':\n",
    "                wd.check_current_traffic('second')\n",
    "                time.sleep(1)\n",
    "        print('Thread #%s stopped' % self.ident)\n",
    "\n",
    "class ServiceExit(Exception):\n",
    "    \"\"\"\n",
    "    Custom exception which is used to trigger the clean exit\n",
    "    of all running threads and the main program.\n",
    "    \"\"\"\n",
    "    pass\n",
    " \n",
    "def service_shutdown(signum, frame):\n",
    "    print('Caught signal %d' % signum)\n",
    "    raise ServiceExit\n",
    "\n",
    "def main():\n",
    " \n",
    "    # Register the signal handlers\n",
    "    signal.signal(signal.SIGTERM, service_shutdown)\n",
    "    signal.signal(signal.SIGINT, service_shutdown)\n",
    " \n",
    "    print('Starting main program')\n",
    " \n",
    "    # Start the job threads\n",
    "    try:\n",
    "#         wd = WatchDog(\"/home/amadeusz/\")\n",
    "        c1 = ClientThread(threadID = 1, mode = 'predict_min')\n",
    "        c1.start()\n",
    "        time.sleep(15)\n",
    "        c2 = ClientThread(threadID = 2, mode = 'predict_sec')\n",
    "        c2.start()\n",
    "        time.sleep(10)\n",
    "        c3 = ClientThread(threadID = 3, mode = 'check_sec')\n",
    "        c3.start()\n",
    "        \n",
    "        c4 = ClientThread(threadID = 3, mode = 'check_min')\n",
    "        c4.start()\n",
    "        \n",
    "        # Keep the main thread running, otherwise signals are ignored.\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    " \n",
    "    except ServiceExit:\n",
    "        # Terminate the running threads.\n",
    "        # Set the shutdown flag on each thread to trigger a clean shutdown of each thread.\n",
    "        c1.shutdown_flag.set()\n",
    "        c2.shutdown_flag.set()\n",
    "        c3.shutdown_flag.set()\n",
    "        # Wait for the threads to close...\n",
    "        c1.join()\n",
    "        c2.join()\n",
    "        c3.join()\n",
    " \n",
    "    print('Exiting main program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wd = WatchDog(\"/home/amadeusz/\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
