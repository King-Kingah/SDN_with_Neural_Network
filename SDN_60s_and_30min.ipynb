{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "All computational and statistics packages have been tested in Anaconda environment (python 3.6)\n",
    "1. Install Anaconda\n",
    "https://www.anaconda.com/download/#linux\n",
    "2. Install Spark and Java 8\n",
    "https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Install \"pyspark\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import pyspark...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import pyspark...')\n",
    "    import pyspark\n",
    "except ImportError:\n",
    "    print('Pyspark import failed...')\n",
    "    print('Installing pyspark in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create Spark Context***\n",
    "If you enqounter any error, try to reconfigure Spark on your machine (link in the first cell - .bashrc file configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    conf = SparkConf().setAppName('SDN')\n",
    "    sc = SparkContext(conf=conf)\n",
    "else:\n",
    "    if sc != None:\n",
    "        sc.stop()\n",
    "    conf = SparkConf().setAppName('SDN')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkConf, SparkContext\n",
    "# conf = SparkConf().setAppName('SDN').setMaster('spark://10.0.2.15:7077').setSparkHome('/usr/local/spark/') \n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SQL context for spark computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import scipy.stats as sts\n",
    "\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.triggers.interval import IntervalTrigger\n",
    "import matplotlib.dates as mdates\n",
    "import json\n",
    "import urllib\n",
    "import logging, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramiko installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import paramiko...\n",
      "Paramiko imported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import paramiko...')\n",
    "    import paramiko\n",
    "    print('Paramiko imported.')\n",
    "except ImportError:\n",
    "    print('Paramiko import failed...')\n",
    "    print('Installing paramiko in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} paramiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCP installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import scp...\n",
      "Scp imported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import scp...')\n",
    "    import scp\n",
    "    print('Scp imported.')\n",
    "except ImportError:\n",
    "    print('Scp import failed...')\n",
    "    print('Installing scp in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras imported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import keras...')\n",
    "    import keras\n",
    "    print('Keras imported.')\n",
    "except ImportError:\n",
    "    print('Keras import failed...')\n",
    "    print('Installing keras in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loginanddownload(hostname,uname,pwd,sfile,tfile):\n",
    "    \"\"\"\n",
    "    Can copy files and directories from PNDa to remote system.\n",
    "    Usage example:\n",
    "        loginanddownload(red_pnda_ip, username, password, remote_folder, local_destination)\n",
    "        loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "    \n",
    "    I am using it only for download full copy of /data folder from pnda VM. There is a dependency of openssh-server\n",
    "    installation on red_pnda VM.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Establishing ssh connection\")\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=uname, password=pwd)\n",
    "    except paramiko.AuthenticationException:\n",
    "        print(\"Authentication failed, please verify your credentials: %s\")\n",
    "    except paramiko.SSHException as sshException:\n",
    "        print(\"Unable to establish SSH connection: %s\" % sshException)\n",
    "    except paramiko.BadHostKeyException as badHostKeyException:\n",
    "        print(\"Unable to verify server's host key: %s\" % badHostKeyException)\n",
    "    except Exception as e:\n",
    "        print(e.args)\n",
    "    try:\n",
    "        print(\"Getting SCP Client\")\n",
    "        scpclient = scp.SCPClient(ssh_client.get_transport())\n",
    "        print(\"Hostname: %s\", hostname)\n",
    "        print(\"source file: %s\", sfile)\n",
    "        print(\"target file: %s\", tfile)\n",
    "        scpclient.get(sfile,tfile, recursive = True)\n",
    "    except scp.SCPException as e:\n",
    "        print(\"Operation error: %s\", e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net:\n",
    "    \"\"\"Neural network class which loads already learned model based on previous traffic from network.\n",
    "        Based on that model we can compute uncertenity intervals. For more details plese refer to:\n",
    "        http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, n_steps, n_length, N, l):\n",
    "        \"\"\"Loads model, initiates variables for uncertenity intervals computation.\"\"\"\n",
    "        # load model from json\n",
    "        json_file = open('{}.json'.format(model_name), 'r')\n",
    "        self.model = json_file.read()\n",
    "        json_file.close()\n",
    "        self.model = model_from_json(self.model)\n",
    "        # load weights into new model\n",
    "        self.model.load_weights(\"{}.h5\".format(model_name))\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "        \"\"\"All below variables are defined based on already learned model.\"\"\"\n",
    "        # define probability for dropout in each layer with weight_decay \n",
    "        # and l - prior length scale, N - number of leanring samples\n",
    "        self.n_steps = n_steps # number of periods on which we based our prediction\n",
    "        self.n_length = n_length # period length in minutes\n",
    "        self.n_input = self.n_length * self.n_steps # define the total minutes to use as input\n",
    "        self.p_dense = 0.05\n",
    "        self.l = l\n",
    "        self.weight_decay = self.l**2/(self.n_input)\n",
    "        self.N = N\n",
    "\n",
    "        # computes tau for variance correction\n",
    "        self.tau = self.l**2 * (1 - self.p_dense) / (2 * self.N * self.weight_decay)\n",
    "        self.tau = self.tau**-1\n",
    "        \n",
    "        \"\"\"For uncertanity prediction we need to approximate our nonlinear neural network\n",
    "            with Gaussian Process. Training phase consisted of randomly dropping neurons from all layers\n",
    "            with propability p_dense and using L2 regularization with weight_decay.\n",
    "            Using random dropout on each layer we can mathematically say that our predictions are \n",
    "            almost normally distributed.\n",
    "            If we want to approximate any GaussianNN we need to use dropout the same way in prediction phase\n",
    "            as we used it in training phase.\n",
    "            In that case we need to slightly manipulate loaded model by enabling dopout in prediction phase.\n",
    "            See lines below.\"\"\"\n",
    "        # defines predict function from keras backend (tensorflow)\n",
    "        # self.model.layers[0].input - take input shape as as it was defined in first layer\n",
    "        # K.learning_phase() - if == 1 (activate droupout from learning phase)\n",
    "        # self.model.layers[-1].output - produce output vector as it was defined (shape)\n",
    "        self.f = K.function([self.model.layers[0].input, K.learning_phase()], [self.model.layers[-1].output])\n",
    "        \n",
    "        # load initial data scaler\n",
    "        if 'second' in model_name:\n",
    "            with open('/home/amadeusz/Documents/SDN_with_Neural_Network/scaler_sec_final.pkl', 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "        else:\n",
    "            with open('/home/amadeusz/Documents/SDN_with_Neural_Network/scaler_final_diff_l.pkl', 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "        \n",
    "        self.predictions = list()\n",
    "        self.uncertenity = list()\n",
    "        self.pred = list()\n",
    "        self.unc = list()\n",
    "        self.unc_up = list()\n",
    "        self.unc_down = list()\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, n_iter=10):\n",
    "        result = np.zeros((n_iter,) + (x.shape[3],))\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            result[i] = self.f([x, 1])[0].reshape(self.n_length)\n",
    "\n",
    "        prediction = result.mean(axis=0)\n",
    "        uncertainty = result.var(axis=0)\n",
    "#         print(uncertainty)\n",
    "#         uncertainty += self.tau\n",
    "        return prediction, uncertainty\n",
    "    \n",
    "    \n",
    "    def forecast(self, history):\n",
    "        # standardize input data\n",
    "        standardized = self.scaler.transform(history)\n",
    "        # flatten data\n",
    "        data = np.array(standardized)\n",
    "        data = data.reshape((data.shape[0], 1))\n",
    "        # retrieve last observations for input data\n",
    "        input_x = data[-self.n_input:, 0]\n",
    "        # reshape into [samples, time steps, rows, cols, channels]\n",
    "        input_x = input_x.reshape((1, self.n_steps, 1, self.n_length, 1))\n",
    "        yhat, uncer = self.predict_with_uncertainty(input_x)\n",
    "        return yhat, uncer\n",
    "    \n",
    "    def make_forecast(self, history):\n",
    "        # reset predictions\n",
    "        self.pred = list()\n",
    "        self.unc = list()\n",
    "        self.predictions = list()\n",
    "        self.uncertenity = list()\n",
    "#         history = [x for x in train]\n",
    "        # predict one period\n",
    "        yhat_sequence, uncert = self.forecast(history)\n",
    "        # store the predictions\n",
    "        self.predictions.append(yhat_sequence)\n",
    "        self.uncertenity.append(uncert)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.uncertenity = np.array(self.uncertenity)\n",
    "        # translate into lists\n",
    "#         test_ = list()\n",
    "        for i in self.predictions.reshape(self.predictions.shape[0]*self.predictions.shape[1]).tolist():\n",
    "            self.pred.append(i)\n",
    "        for i in self.uncertenity.reshape(self.uncertenity.shape[0]*self.uncertenity.shape[1]).tolist():\n",
    "            self.unc.append(i)\n",
    "            \n",
    "        self.unc = np.sqrt(self.scaler.inverse_transform(np.array(self.unc))) # sigma\n",
    "        \n",
    "        \n",
    "#         self.unc_up = self.scaler.inverse_transform(np.array(self.pred) + np.array(self.unc))\n",
    "#         self.unc_down = self.scaler.inverse_transform(np.array(self.pred) - np.array(self.unc))\n",
    "        \n",
    "#         self.unc_up += self.tau\n",
    "#         self.unc_down -= self.tau\n",
    "        \n",
    "        self.pred = self.scaler.inverse_transform(self.pred)\n",
    "#         self.unc = self.scaler.inverse_transform(self.unc)\n",
    "        \n",
    "#         return self.pred, self.unc_up, self.unc_down\n",
    "        return self.pred, self.unc, self.tau\n",
    "#         for i in test.reshape(test.shape[0]*test.shape[1]).tolist():\n",
    "#             test_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \"\"\"\n",
    "    Prediction class - class for preprocessing data from red_pnda.\n",
    "    Not full variables are in use (this is a changed copy of Lecturer shared file)\n",
    "    \n",
    "    Prediction class takes exacly one argument - bytes (network traffic data)\n",
    "    It is further processed and returned in other format.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, bytes_, dates, network_min, network_sec):\n",
    "        self.bytes = bytes_\n",
    "        self.dates = dates\n",
    "        self.network_min = network_min\n",
    "        self.network_sec = network_sec\n",
    "#         self.predictions_min = list()\n",
    "#         self.uncertenity_up_min = list()\n",
    "#         self.uncertenity_down_min = list()\n",
    "#         self.predictions_sec = list()\n",
    "#         self.uncertenity_up_sec = list()\n",
    "#         self.uncertenity_down_sec = list()\n",
    "        self.predictions_min = list()\n",
    "        self.sigma_min = list()\n",
    "        self.tau_min = None\n",
    "        self.predictions_sec = list()\n",
    "        self.sigma_sec = list()\n",
    "        self.tau_sec = None\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def prepare_data_for_prediction(self, interval = 'minute'):\n",
    "        \"\"\"\n",
    "        From RAW data compute time dependency (x) and bandwidth (y).\n",
    "        Find mean traffic value for each minute.\n",
    "        \"\"\"\n",
    "        self.data = self.bytes\n",
    "        self.data = pd.DataFrame(data=self.data, columns=['traffic'])\n",
    "        self.data = self.data.set_index(pd.DatetimeIndex(self.dates))\n",
    "#         print(self.train.head())\n",
    "#         print('Shape: {}'.format(self.train.shape))\n",
    "        if interval == 'minute':\n",
    "            self.data = self.data.groupby(pd.Grouper(freq='Min')).mean()\n",
    "        elif interval == 'second':\n",
    "            pass\n",
    "#         print(self.train.head())\n",
    "#         print('Shape: {}'.format(self.train.shape))\n",
    "\n",
    "    def proceed_prediction(self, interval = 'min'):\n",
    "        \"\"\"\n",
    "        Start data preprocessing.\n",
    "        \"\"\"\n",
    "        if interval == 'minute':\n",
    "            self.prepare_data_for_prediction('minute')\n",
    "#             self.predictions_min, self.uncertenity_up_min, self.uncertenity_down_min = self.network_min.make_forecast([x for x in self.data.values])\n",
    "            self.predictions_min, self.sigma_min, self.tau_min = self.network_min.make_forecast([x for x in self.data.values])\n",
    "\n",
    "        elif interval == 'second':\n",
    "            self.prepare_data_for_prediction('second')\n",
    "#             self.predictions_sec, self.uncertenity_up_sec, self.uncertenity_down_sec = self.network_sec.make_forecast([x for x in self.data.values])\n",
    "            self.predictions_sec, self.sigma_sec, self.tau_sec = self.network_sec.make_forecast([x for x in self.data.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatchDog:\n",
    "    \"\"\"\n",
    "    WatchDog class - extracts bandwidth information from collected flow and port data.\n",
    "    (Not all variables are in use)\n",
    "    \n",
    "    self.dpid - particular switch identification number\n",
    "    self.port_no - particular port number on the switch\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):    \n",
    "        self.interval = 60 # co ile sekund sprawdzam czy nie ma anomalii\n",
    "        self.network_min = None\n",
    "        self.network_sec = None\n",
    "        self.unusual = 0\n",
    "        self.normal_work = True\n",
    "        self.train_data = None\n",
    "        self.pred = None\n",
    "        self.pi = None\n",
    "        self.current_stats = None\n",
    "#         self.sched = BackgroundScheduler()\n",
    "#         self.sched.start()\n",
    "        self.dpid = 2\n",
    "        self.port_no = 2\n",
    "        self.resampled = None\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def get_last_hour_stats(self):\n",
    "        \"\"\"\n",
    "        Currently not in use.\n",
    "        \"\"\"\n",
    "        last_hour_time = datetime.now() - timedelta(hours = 1)\n",
    "        year = int(last_hour_time.strftime(\"%Y\"))\n",
    "        month = int(last_hour_time.strftime(\"%m\"))\n",
    "        day = int(last_hour_time.strftime(\"%d\"))\n",
    "        hour = int(last_hour_time.strftime(\"%H\"))\n",
    "        self.train_data = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                             \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "        \n",
    "    def get_previous_stats(self):\n",
    "        \"\"\"\n",
    "        Gather statistics from particular hour/day.\n",
    "        You can change dates based on your gathered data.\n",
    "        \"\"\"\n",
    "#         year=\"2019\"\n",
    "#         month=\"1\"\n",
    "#         day=\"4\"\n",
    "#         hour=0\n",
    "        year = int(time.strftime(\"%Y\"))\n",
    "        month = int(time.strftime(\"%m\"))\n",
    "        day = int(time.strftime(\"%d\")) - 4\n",
    "        hour = int(time.strftime(\"%H\"))\n",
    "        roznica = 0\n",
    "        d = list()\n",
    "        if hour - 5 < 0: # jezeli 5 godzin temu to byl inny dzien\n",
    "            day -= 1 # zmien dzien na wczoraj\n",
    "            roznica = np.abs(hour - 5) # policz roznice w godzinach\n",
    "            hour = 24 + (hour - 5) # policz godzine jaka byla 5 godzin temu, skaluj do wczorajszej 24-ki\n",
    "            # pobierz dane z konkretnej godziny wczoraj\n",
    "            data = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                            \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "            for i in range(1,roznica): # pobieraj dane do konca wczorajszego dnia\n",
    "                h = hour + i\n",
    "                d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                                  \"/day=\"+str(day)+\"/hour=\"+str(h)+\"/dump.json\"))\n",
    "#                 print('h = {}'.format(h))\n",
    "\n",
    "            for i in range(int(time.strftime(\"%H\"))+1): # pobieraj dane od polnocy do biezacej godziny\n",
    "                h = i\n",
    "                d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                                  \"/day=\"+str(day+1)+\"/hour=\"+str(h)+\"/dump.json\"))\n",
    "#                 print('h = {}'.format(h))\n",
    "        else: # jezeli 5 godzin temu nadal bylo dzisiaj\n",
    "            hour -= 5 # cofnij sie o 5 godzin i pobierz dane\n",
    "            data = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                            \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "            for i in range(hour+1, int(time.strftime(\"%H\"))+1): # pobieraj dane do biezacej godziny\n",
    "                h = i\n",
    "                d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                                  \"/day=\"+str(day)+\"/hour=\"+str(h)+\"/dump.json\"))\n",
    "            print(d[1])\n",
    "\n",
    "        # zcal wszystkie pobrane dane z 5-6 godzin\n",
    "        for i in range(0,len(d)):\n",
    "            data = data.unionAll(d[i])            \n",
    "        self.train_data = data\n",
    "        \n",
    "        return hour\n",
    "    \n",
    "    def get_current_stats(self):\n",
    "        \"\"\"\n",
    "        Currently not in use.\n",
    "        \"\"\"\n",
    "        year = int(time.strftime(\"%Y\"))\n",
    "        month = int(time.strftime(\"%m\"))\n",
    "        day = int(time.strftime(\"%d\")) - 4 # do usuniecia 2 !!!!!!!!!!!!!!!!!!!!!!!\n",
    "        hour = int(time.strftime(\"%H\"))\n",
    "        loginanddownload('192.168.57.5', 'pnda', 'pnda', '/data/year={}/month={}/day={}/hour={}/'.format(year, \n",
    "                                                                                                         month, \n",
    "                                                                                                         day,\n",
    "                                                                                                         hour),\n",
    "                         '/home/amadeusz/data/year={}/month={}/day={}/'.format(year, \n",
    "                                                                               month, \n",
    "                                                                               day, \n",
    "                                                                               hour))\n",
    "        \n",
    "        self.current_stats = sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "                                             \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "#         self.current_stats = sqlContext.read.json(\"data/year=\"+str(year)+\"/month=\"+str(month)+\n",
    "#                                              \"/day=\"+str(day)+\"/hour=\"+str(hour)+\"/dump.json\")\n",
    "    \n",
    "    def get_port_stats(self,data,dpid,port_no):\n",
    "        \"\"\"\n",
    "        Gathers data from specific port on the switch.\n",
    "        \"\"\"\n",
    "        port = data.filter((data['origin']=='port_stats') & \n",
    "                           (data['switch_id']==dpid) & \n",
    "                           (data['port_no']==port_no)).orderBy('timestamp')\n",
    "        port = port.toPandas()\n",
    "        ts = pd.Series(port['timestamp'].astype(int))\n",
    "        ts = pd.to_datetime(ts, unit='s')\n",
    "        index = pd.DatetimeIndex(ts)\n",
    "        raw_data = pd.Series(port['tx_bytes'].values, index=index)\n",
    "        return raw_data, port\n",
    "    \n",
    "    def get_last_tput(self, dpid, port_no, interval):\n",
    "        \"\"\"\n",
    "        Pobieram minute ostatniego ruchu do analizy\n",
    "        \"\"\"\n",
    "        if interval == 'minute':\n",
    "            # pobieram 13 ostatnich probek (jedna probka co 5 secund czyli chce miec 60 sekund ruchu)\n",
    "            last_minute = self.current_stats.filter((self.current_stats['origin']=='port_stats') & \n",
    "                               (self.current_stats['switch_id']==dpid) & \n",
    "                               (self.current_stats['port_no']==port_no)).orderBy('timestamp', ascending=False).limit(13)\n",
    "            last_minute = last_minute.toPandas()\n",
    "            bytes_ = last_minute['tx_bytes'].astype(int)\n",
    "            time_ = last_minute['timestamp'].astype(int)\n",
    "            tput = (bytes_[0]-bytes_.iloc[-1])/(time_[0]-time_.iloc[-1]) # licze roznice w ruchu jaki byl 60s temu i teraz i dziele przez czas\n",
    "        elif interval == 'second':\n",
    "            # pobieram 2 ostatnie probki (jedna probka co 5 secund czyli chce miec 5 sekund ruchu, wyciagam srednia na sekunde)\n",
    "            last_minute = self.current_stats.filter((self.current_stats['origin']=='port_stats') & \n",
    "                               (self.current_stats['switch_id']==dpid) & \n",
    "                               (self.current_stats['port_no']==port_no)).orderBy('timestamp', ascending=False).limit(2)\n",
    "            last_minute = last_minute.toPandas()\n",
    "            bytes_ = last_minute['tx_bytes'].astype(int)\n",
    "            time_ = last_minute['timestamp'].astype(int)\n",
    "            tput = (bytes_[0]-bytes_.iloc[-1])/(time_[0]-time_.iloc[-1]) # licze roznice w ruchu jaki byl w czasie 2s\n",
    "       \n",
    "        return tput*8/1e6 #Mbps (licze/zamieniam na Mbps)\n",
    "        \n",
    "    def resample_port_stats (self,raw_data, port, start):\n",
    "        \"\"\"\n",
    "        Resamples data into correct date format and frequency.\n",
    "        \"\"\"\n",
    "        raw_data = raw_data[~raw_data.index.duplicated(keep='first')]\n",
    "        resampled_data = raw_data.resample('s').interpolate()\n",
    "        resampled_data = [(y - x) for x,y in zip(resampled_data.values,resampled_data.values[1:])]\n",
    "        ts_resampled = pd.Series(range(len(resampled_data))) + start*60*60\n",
    "        ts_resampled= pd.to_datetime(ts_resampled, unit='s')\n",
    "        return resampled_data, ts_resampled\n",
    "    \n",
    "    \n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    def check_current_traffic(self, mode):\n",
    "#         loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "        hour = int(time.strftime(\"%H\"))\n",
    "        minute = int(time.strftime(\"%M\"))\n",
    "        second = int(time.strftime(\"%S\"))\n",
    "        if mode == 'minute':\n",
    "\n",
    "            # upper and lower limits for minute prediction\n",
    "            up_min = self.pred_check_data_min.loc[(self.pred_check_data_min['hour'] == hour) & \n",
    "                                                  (self.pred_check_data_min['minute'] == minute)]['up'].values[0]\n",
    "            down_min = self.pred_check_data_min.loc[(self.pred_check_data_min['hour'] == hour) & \n",
    "                                                    (self.pred_check_data_min['minute'] == minute)]['down'].values[0]\n",
    "            print('up: {}'.format(up_min))\n",
    "            print('down: {}'.format(down_min))\n",
    "            self.get_current_stats() # pobierz dane z teraz\n",
    "            cur_tput_min = self.get_last_tput(self.dpid, self.port_no, 'minute') # wyodrebnij tylko dane z danego portu na switchu\n",
    "            _ = system('clear') \n",
    "            print('tput = {}'.format(cur_tput_min))\n",
    "            if cur_tput_min > up_min or cur_tput_min < down_min:\n",
    "                print('Anomaly detected!!! (minutes interval)')\n",
    "                self.normal_work = False\n",
    "            else:\n",
    "                print('Normal traffic... (minutes interval)')\n",
    "                self.normal_work = True\n",
    "        elif mode == 'second':\n",
    "            # upper and lower limits for second prediction\n",
    "            up_sec = self.pred_check_data_sec.loc[(self.pred_check_data_sec['hour'] == hour) & \n",
    "                                                  (self.pred_check_data_sec['minute'] == minute) &\n",
    "                                                  (pred_check_data_sec['second'] == second)]['up'].values[0]\n",
    "            down_sec = self.pred_check_data_sec.loc[(self.pred_check_data_sec['hour'] == hour) & \n",
    "                                                    (self.pred_check_data_sec['minute'] == minute) &\n",
    "                                                    (pred_check_data_sec['second'] == second)]['down'].values[0]\n",
    "\n",
    "            print('up: {}'.format(up_sec))\n",
    "            print('down: {}'.format(down_sec))\n",
    "            self.get_current_stats() # pobierz dane z teraz\n",
    "            cur_tput_sec = self.get_last_tput(self.dpid, self.port_no, 'second') # wyodrebnij tylko dane z danego portu na switchu\n",
    "            _ = system('clear') \n",
    "            print('tput = {}'.format(cur_tput_sec))\n",
    "            if (cur_tput_sec > up_sec or cur_tput_sec < down_sec):\n",
    "                self.unusual += 1\n",
    "                if self.unusual >= 50:\n",
    "                    print('Anomaly detected!!! (seconds interval)')\n",
    "                    self.normal_work = False\n",
    "            else:\n",
    "                self.unusual = 0\n",
    "                print('Normal traffic... (seconds interval)')\n",
    "                self.normal_work = True\n",
    "        \n",
    "            \n",
    "#         if self.unusual > 2 and self.normal_work == True:\n",
    "#             self.change_interval(5)\n",
    "#             self.normal_work = False\n",
    "#             print('changing interval to 5s')\n",
    "#         elif self.unusual == 0 and self.normal_work == False:\n",
    "#             self.change_interval(300)\n",
    "#             self.normal_work = True\n",
    "#             print('changing interval to 300s')\n",
    "            \n",
    "    # wystartuj observer i monitoruj/zmieniaj interval ??\n",
    "    def start_stats_observer(self):\n",
    "        self.sched.add_job(self.check_current_traffic, 'interval', seconds=self.interval)\n",
    "\n",
    "    def change_job_interval(self,interval):\n",
    "        print(\"Rescheduling stat request to %i seconds\", interval)\n",
    "        for s in self.sched.get_jobs():\n",
    "            print('rescheduling job %s', s.id)\n",
    "            it = IntervalTrigger(seconds=interval)\n",
    "            self.sched.reschedule_job(s.id, trigger=it)\n",
    "\n",
    "    def change_interval(self, interval):\n",
    "        print(\"changing interval to %i\" % interval)\n",
    "        self.send_stats_interval(interval)\n",
    "        self.change_job_interval(interval)\n",
    "        \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def proceed_prediction(self, enable_plot, mode, plot_prediction):\n",
    "        \"\"\"\n",
    "        Start preprocessing traffic data.\n",
    "        \n",
    "        If you want to download data to your localhost just uncomment first line.\n",
    "        This is not necessary when you have all the data stored locally already.\n",
    "        \"\"\"\n",
    "#         loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "        year = int(time.strftime(\"%Y\"))\n",
    "        month = int(time.strftime(\"%m\"))\n",
    "        day = int(time.strftime(\"%d\"))\n",
    "#         loginanddownload('192.168.57.5', 'pnda', 'pnda', '/data/year={}/month={}/day={}/'.format(year, month, day),\n",
    "#                          '/home/amadeusz/data/year={}/month={}/'.format(year, month, day))\n",
    "#         self.get_current_stats()\n",
    "        if self.network_min is None and self.network_sec is None:\n",
    "            self.network_min = Neural_Net('/home/amadeusz/Documents/SDN_with_Neural_Network/model_standardise_with_l_higher_wd_bayasian_RNN', 10, 30, 1440*6, 1e-1) # loads neural network model minutes\n",
    "            self.network_sec = Neural_Net('/home/amadeusz/Documents/SDN_with_Neural_Network/model_standardise_seconds_l_bayasian_RNN', 10, 60, 86400, 1e-1) # loads neural network model seconds\n",
    "        \n",
    "        start = self.get_previous_stats() # pobierz 5 godzin ruchu od teraz wstecz\n",
    "        data, port_stats = self.get_port_stats(self.train_data, self.dpid, self.port_no) # wydziel ruch z danego portu\n",
    "        resampled_data, ts_resampled = self.resample_port_stats(data, port_stats, start) # przygotuj dane\n",
    "        \n",
    "        # test (jezeli robie bez wlaczonej kafki, musze usunac dopelnienie do godziny ruchu bo mam juz pobrany)\n",
    "#         minute = 60 - int(time.strftime(\"%M\"))\n",
    "#         resampled_data = resampled_data[:-minute*60]\n",
    "#         ts_resampled = ts_resampled[:-minute*60]\n",
    "    \n",
    "        self.ts_resampled = ts_resampled # store time vector\n",
    "        self.resampled = [x*8/1e6 for x in resampled_data] # zamien na Mbps (nadal w przedzialach sekundowych)\n",
    "\n",
    "        prediction = Prediction(self.resampled, ts_resampled, self.network_min, self.network_sec) # prepare data for prediction\n",
    "        if mode == 'minute':\n",
    "            prediction.proceed_prediction('minute') # make 30 minutes prediction\n",
    "        elif mode == 'second':\n",
    "            prediction.proceed_prediction('second') # make 60 seconds prediction\n",
    "        \n",
    "        # plotting part (plots 5 hours back and predicted mean with uncertenity)\n",
    "        if enable_plot:\n",
    "            if mode == 'minute':\n",
    "                fig = plt.figure(figsize=(15,8))\n",
    "                ax = fig.add_subplot(111)\n",
    "                # resample 5 hour data from seconds into minutes\n",
    "                ixy = pd.DataFrame(data=[x*8/1e6 for x in resampled_data], columns=['traffic'])\n",
    "                ixy = ixy.set_index(pd.DatetimeIndex(ts_resampled))\n",
    "                ixy = ixy.groupby(pd.Grouper(freq='Min')).mean()\n",
    "                # \n",
    "                new_pred = prediction.predictions_min.tolist()\n",
    "#                 new_unc_up = prediction.uncertenity_up_min.tolist()\n",
    "#                 new_unc_down = prediction.uncertenity_down_min.tolist()\n",
    "                new_sigma_min = prediction.sigma_min.tolist()\n",
    "                for position, i in enumerate(ixy.values.tolist()):\n",
    "                    new_pred.insert(position,i[0])\n",
    "#                     new_unc_up.insert(position,i[0])\n",
    "#                     new_unc_down.insert(position,i[0])\n",
    "                    new_sigma_min.insert(position,i[0])\n",
    "                # creates datetime range for plots\n",
    "                if self.ts_resampled.iloc[-1].minute + 30 < 60:\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[0].year,\n",
    "                                                                       self.ts_resampled.iloc[0].month,\n",
    "                                                                       self.ts_resampled.iloc[0].day,\n",
    "                                                                       self.ts_resampled.iloc[0].hour,\n",
    "                                                                       self.ts_resampled.iloc[0].minute), \n",
    "                                      end='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     self.ts_resampled.iloc[-1].day,\n",
    "                                                                     self.ts_resampled.iloc[-1].hour,\n",
    "                                                                     self.ts_resampled.iloc[-1].minute+30), \n",
    "                                      freq=\"1min\")\n",
    "                else:\n",
    "                    print('jednak tutaj weszlo')\n",
    "                    if self.ts_resampled.iloc[-1].hour+1 >= 24:\n",
    "                        hour = 0\n",
    "                        day = self.ts_resampled.iloc[-1].day + 1\n",
    "                    else:\n",
    "                        hour = self.ts_resampled.iloc[-1].hour+1\n",
    "                        day = self.ts_resampled.iloc[-1].day\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[0].year,\n",
    "                                                                       self.ts_resampled.iloc[0].month,\n",
    "                                                                       self.ts_resampled.iloc[0].day,\n",
    "                                                                       self.ts_resampled.iloc[0].hour,\n",
    "                                                                       self.ts_resampled.iloc[0].minute), \n",
    "                                      end='{}-{}-{} {}:{}:00'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     day,\n",
    "                                                                     hour,\n",
    "                                                                     (self.ts_resampled.iloc[-1].minute+30) - 60), \n",
    "                                      freq=\"1min\")\n",
    "                x2 = pd.date_range(start='{}'.format(self.ts_resampled.iloc[0]), \n",
    "                                      end='{}'.format(self.ts_resampled.iloc[-1]), \n",
    "                                      freq=\"1min\")\n",
    "\n",
    "                self.pred_check_data_min = pd.DataFrame(data=np.zeros(len(new_sigma_min)), \n",
    "                                                    columns=['zeros'])\n",
    "\n",
    "#                 self.pred_check_data_min['up'] = new_unc_up\n",
    "#                 self.pred_check_data_min['down'] = new_unc_down\n",
    "                self.pred_check_data_min['sigma'] = new_sigma_min\n",
    "                self.pred_check_data_min['hour'] = [p.hour for p in pd.to_datetime(x1)]\n",
    "                self.pred_check_data_min['minute'] = [p.minute for p in pd.to_datetime(x1)]\n",
    "                \n",
    "                if plot_prediction:\n",
    "                    ax.plot(x1, new_pred, 'g') # plot prediction mean\n",
    "#                     ax.fill_between(x1, new_unc_up, new_unc_down, facecolor='yellow') # plot prediction uncertenity interval\n",
    "                    \n",
    "                    new_sigma_min = prediction.sigma_min.tolist()\n",
    "                    new_4sigma_min_up = np.asarray(new_pred) + (np.asarray(new_sigma_min) + prediction.tau_min)*4\n",
    "                    new_4sigma_min_up = new_4sigma_min_up.tolist()\n",
    "                    new_4sigma_min_down = np.asarray(new_pred) - (np.asarray(new_sigma_min) + prediction.tau_min)*4\n",
    "                    new_4sigma_min_down = new_4sigma_min_down.tolist()\n",
    "                    new_3sigma_min_up = np.asarray(new_pred) + (np.asarray(new_sigma_min) + prediction.tau_min)*3\n",
    "                    new_3sigma_min_up = new_3sigma_min_up.tolist()\n",
    "                    new_3sigma_min_down = np.asarray(new_pred) - (np.asarray(new_sigma_min) + prediction.tau_min)*3\n",
    "                    new_3sigma_min_down = new_3sigma_min_down.tolist()\n",
    "                    new_sigma_min_up = np.asarray(new_pred) + (np.asarray(new_sigma_min) + prediction.tau_min)\n",
    "                    new_sigma_min_up = new_sigma_min_up.tolist()\n",
    "                    new_sigma_min_down = np.asarray(new_pred) - (np.asarray(new_sigma_min) + prediction.tau_min)\n",
    "                    new_sigma_min_down = new_sigma_min_down.tolist()\n",
    "                    \n",
    "                    for position, i in enumerate(ixy.values.tolist()):\n",
    "                        new_4sigma_min_up.insert(position,i[0])\n",
    "                        new_4sigma_min_down.insert(position,i[0])\n",
    "                        new_3sigma_min_up.insert(position,i[0])\n",
    "                        new_3sigma_min_down.insert(position,i[0])\n",
    "                        new_sigma_min_up.insert(position,i[0])\n",
    "                        new_sigma_min_down.insert(position,i[0])\n",
    "                        \n",
    "                    # 4*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_4sigma_min_up, \n",
    "                                    new_4sigma_min_down, \n",
    "                                    facecolor='yellow', alpha=0.3)\n",
    "                    # 3*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_3sigma_min_up, \n",
    "                                    new_3sigma_min_down, \n",
    "                                    facecolor='orange', alpha=0.3)\n",
    "                    # sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    new_sigma_min_up, \n",
    "                                    new_sigma_min_down, \n",
    "                                    facecolor='blue', alpha=0.3)\n",
    "                ax.plot(x2, ixy.values, 'r') # plot 5 hours back\n",
    "\n",
    "                ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "                \n",
    "            elif mode == 'second':\n",
    "                # seconds part\n",
    "                fig = plt.figure(figsize=(15,8))\n",
    "                ax = fig.add_subplot(111)\n",
    "                # resample 5 hour data from seconds into minutes\n",
    "                ixy = pd.DataFrame(data=[x*8/1e6 for x in resampled_data][-600:], columns=['traffic'])\n",
    "                ixy = ixy.set_index(pd.DatetimeIndex(ts_resampled[-600:]))\n",
    "                # \n",
    "                new_pred = prediction.predictions_sec.tolist()\n",
    "#                 new_unc_up = prediction.uncertenity_up_sec.tolist()\n",
    "#                 new_unc_down = prediction.uncertenity_down_sec.tolist()\n",
    "                new_sigma_sec = prediction.sigma_sec.tolist()\n",
    "                for position, i in enumerate(ixy.values.tolist()):\n",
    "                    new_pred.insert(position,i[0])\n",
    "#                     new_unc_up.insert(position,i[0])\n",
    "#                     new_unc_down.insert(position,i[0])\n",
    "                    new_sigma_sec.insert(position,i[0])\n",
    "                # creates datetime range for plots\n",
    "                if self.ts_resampled.iloc[-1].minute + 1 < 60:\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-600].year,\n",
    "                                                                       self.ts_resampled.iloc[-600].month,\n",
    "                                                                       self.ts_resampled.iloc[-600].day,\n",
    "                                                                       self.ts_resampled.iloc[-600].hour,\n",
    "                                                                       self.ts_resampled.iloc[-600].minute,\n",
    "                                                                       self.ts_resampled.iloc[-600].second), \n",
    "                                      end='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     self.ts_resampled.iloc[-1].day,\n",
    "                                                                     self.ts_resampled.iloc[-1].hour,\n",
    "                                                                     self.ts_resampled.iloc[-1].minute+1,\n",
    "                                                                     self.ts_resampled.iloc[-1].second), \n",
    "                                      freq=\"1S\")\n",
    "                else:\n",
    "                    print('jednak tutaj weszlo')\n",
    "                    if self.ts_resampled.iloc[-1].hour+1 >= 24:\n",
    "                        hour = 0\n",
    "                        day = self.ts_resampled.iloc[-1].day + 1\n",
    "                    else:\n",
    "                        hour = self.ts_resampled.iloc[-1].hour+1\n",
    "                        day = self.ts_resampled.iloc[-1].day\n",
    "                    x1 = pd.date_range(start='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-600].year,\n",
    "                                                                       self.ts_resampled.iloc[-600].month,\n",
    "                                                                       self.ts_resampled.iloc[-600].day,\n",
    "                                                                       self.ts_resampled.iloc[-600].hour,\n",
    "                                                                       self.ts_resampled.iloc[-600].minute,\n",
    "                                                                       self.ts_resampled.iloc[-600].second), \n",
    "                                      end='{}-{}-{} {}:{}:{}'.format(self.ts_resampled.iloc[-1].year,\n",
    "                                                                     self.ts_resampled.iloc[-1].month,\n",
    "                                                                     day,\n",
    "                                                                     hour,\n",
    "                                                                     '00',\n",
    "                                                                     self.ts_resampled.iloc[-1].second), \n",
    "                                      freq=\"1S\")\n",
    "                x2 = pd.date_range(start='{}'.format(self.ts_resampled.iloc[-600]), \n",
    "                                      end='{}'.format(self.ts_resampled.iloc[-1]), \n",
    "                                      freq=\"1S\")\n",
    "\n",
    "                self.pred_check_data_sec = pd.DataFrame(data=np.zeros(len(new_sigma_sec)), \n",
    "                                                    columns=['zeros'])\n",
    "\n",
    "#                 self.pred_check_data_sec['up'] = new_unc_up\n",
    "#                 self.pred_check_data_sec['down'] = new_unc_down\n",
    "                self.pred_check_data_sec['sigma'] = new_sigma_sec\n",
    "                self.pred_check_data_sec['hour'] = [p.hour for p in pd.to_datetime(x1)]\n",
    "                self.pred_check_data_sec['minute'] = [p.minute for p in pd.to_datetime(x1)]\n",
    "                self.pred_check_data_sec['second'] = [p.second for p in pd.to_datetime(x1)]\n",
    "                \n",
    "                if plot_prediction:\n",
    "                    ax.plot(x1, new_pred, 'g') # plot prediction mean\n",
    "#                     ax.fill_between(x1, new_unc_up, new_unc_down, facecolor='yellow') # plot prediction uncertenity interval\n",
    "                    # 4*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    np.asarray(new_pred) + (np.asarray(new_sigma_sec) + prediction.tau_sec)*4, \n",
    "                                    np.asarray(new_pred) - (np.asarray(new_sigma_sec) + prediction.tau_sec)*4, \n",
    "                                    facecolor='yellow', alpha=0.3)\n",
    "                    # 3*sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    np.asarray(new_pred) + (np.asarray(new_sigma_sec) + prediction.tau_sec)*3, \n",
    "                                    np.asarray(new_pred) - (np.asarray(new_sigma_sec) + prediction.tau_sec)*3, \n",
    "                                    facecolor='orange', alpha=0.3)\n",
    "                    # sigma\n",
    "                    ax.fill_between(x1, \n",
    "                                    np.asarray(new_pred) + (np.asarray(new_sigma_sec) + prediction.tau_sec), \n",
    "                                    np.asarray(new_pred) - (np.asarray(new_sigma_sec) + prediction.tau_sec), \n",
    "                                    facecolor='blue', alpha=0.3)\n",
    "                ax.plot(x2, ixy.values, 'r') # plot 5 hours back\n",
    "\n",
    "                ax.set_ylabel(\"Tput [Mbps]\")\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.ts_resampled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wd = WatchDog(\"/home/amadeusz/\")\n",
    "wd.check_current_traffic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load the data***\n",
    "Preprocessed data will be stored in wd.resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing ssh connection\n",
      "Getting SCP Client\n",
      "Hostname: %s 192.168.57.5\n",
      "source file: %s /data\n",
      "target file: %s /home/amadeusz/\n"
     ]
    }
   ],
   "source": [
    "loginanddownload('192.168.57.5', 'pnda', 'pnda', '/data', '/home/amadeusz/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amadeusz/anaconda3/envs/nowe/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator StandardScaler from version 0.20.2 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/amadeusz/data/year=2019/month=1/day=24/hour=0/dump.json;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nowe/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nowe/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o415.json.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/amadeusz/data/year=2019/month=1/day=24/hour=0/dump.json;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5f012c1f6cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWatchDog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/amadeusz/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproceed_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menable_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'minute'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-281fc7d53746>\u001b[0m in \u001b[0;36mproceed_prediction\u001b[0;34m(self, enable_plot, mode, plot_prediction)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/amadeusz/Documents/SDN_with_Neural_Network/model_standardise_seconds_l_bayasian_RNN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m86400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loads neural network model seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_previous_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pobierz 5 godzin ruchu od teraz wstecz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_port_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport_no\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# wydziel ruch z danego portu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mresampled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample_port_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# przygotuj dane\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-281fc7d53746>\u001b[0m in \u001b[0;36mget_previous_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 d.append(sqlContext.read.json(self.data_path+\"data/year=\"+str(year)+\"/month=\"+str(month)+\n\u001b[0;32m---> 69\u001b[0;31m                                                   \"/day=\"+str(day+1)+\"/hour=\"+str(h)+\"/dump.json\"))\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;31m#                 print('h = {}'.format(h))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# jezeli 5 godzin temu nadal bylo dzisiaj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nowe/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nowe/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nowe/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/amadeusz/data/year=2019/month=1/day=24/hour=0/dump.json;'"
     ]
    }
   ],
   "source": [
    "# please specify path to data folder\n",
    "%matplotlib inline\n",
    "wd = WatchDog(\"/home/amadeusz/\")\n",
    "start = time.time()\n",
    "wd.proceed_prediction(enable_plot = True, mode = 'minute', plot_prediction = True)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "start = time.time()\n",
    "wd.proceed_prediction(enable_plot = True, mode = 'second', plot_prediction = True)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you have preprocessed traffic data stored in var series. Data is in Mbps.\n",
    "series = [x*8/1e6 for x in wd.resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    wd.check_current_traffic()\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test\n",
    "\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=n_lag, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(8, kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(n_seq, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "\n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "\n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(series)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series[off_s]] + forecasts[i]\n",
    "        plt.plot(xaxis, yaxis, color='red')\n",
    "\n",
    "# load dataset\n",
    "series = series = [x*8/1e6 for x in wd.resampled]\n",
    "# configure\n",
    "n_lag = 100\n",
    "n_seq = 100\n",
    "n_test = 10000\n",
    "n_epochs = 40\n",
    "n_batch = 1000\n",
    "n_neurons = 64\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "# fit model\n",
    "model = wider_model()\n",
    "# evaluate model with standardized dataset\n",
    "history = model.fit(train[:,0:-n_seq], train[:,-n_seq:], epochs=40, batch_size=1000, verbose=1)\n",
    "# Predict test dataset without confidence interval\n",
    "forecasts = model.predict(test[:,0:-n_seq], batch_size=1000)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[-n_seq:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# # plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network part (only one step ahead prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem\n",
    "# def timeseries_to_supervised(data, lag=1):\n",
    "#     df = pd.DataFrame(data)\n",
    "#     columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "#     columns.append(df)\n",
    "#     df = pd.concat(columns, axis=1)\n",
    "#     df.fillna(0, inplace=True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    " \n",
    "# # create a differenced series\n",
    "# def difference(dataset, interval=1):\n",
    "#     diff = list()\n",
    "#     for i in range(interval, len(dataset)):\n",
    "#         value = dataset[i] - dataset[i - interval]\n",
    "#         diff.append(value)\n",
    "#     return pd.Series(diff)\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    " \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    " \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]\n",
    " \n",
    "# load dataset\n",
    "series = [x*8/1e6 for x in wd.resampled]\n",
    "\n",
    "# transform data to be stationary\n",
    "diff_values = difference(series, 1)\n",
    " \n",
    "# rescale values to -1, 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "diff_values = diff_values.values\n",
    "diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "scaled_values = scaler.fit_transform(diff_values)   \n",
    "\n",
    "# transform data to be supervised learning\n",
    "# supervised = timeseries_to_supervised(diff_values, 100)\n",
    "n_lag = 10\n",
    "n_seq = 3\n",
    "supervised = series_to_supervised(diff_values.tolist(), n_lag, n_seq)\n",
    "supervised_values = supervised.values\n",
    " \n",
    "# # split data into train and test-sets\n",
    "# train, test = supervised_values[0:-2000], supervised_values[-2000:]\n",
    " \n",
    "# # transform the scale of the data\n",
    "# scaler, train_scaled, test_scaled = scale(train, test)\n",
    "train_scaled, test_scaled = supervised_values[0:-2000], supervised_values[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create NN model in Keras***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=n_lag, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(8, kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(n_seq, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "model = wider_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "history = model.fit(train_scaled[:,0:-n_seq], train_scaled[:,-n_seq:], epochs=40, batch_size=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('Number of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test dataset without confidence interval\n",
    "predictions = model.predict(test_scaled[:,0:-n_seq], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "inverted = inverse_transform(series, predictions, scaler, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inverted)# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, predictions, scaler, 2000+2)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, 2000+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling, differencing\n",
    "pred = list()\n",
    "for i in range(len(test_scaled)):\n",
    "    X, y = test_scaled[i, 0:-n_seq], test_scaled[i, -n_seq:]\n",
    "    # invert scaling\n",
    "    yhat = invert_scale(scaler, X, predictions[i])\n",
    "    # invert differencing\n",
    "    yhat = inverse_difference(series, yhat, len(test_scaled)+1-i)\n",
    "    # store forecast\n",
    "    pred.append(yhat)\n",
    "    expected = series[len(train) + i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(series[-2000:], 'b')\n",
    "plt.plot(inverted, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance \n",
    "rmse = np.sqrt(mean_squared_error(series[-2000:-1], pred[1:]))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prediction with confidence interval***  \n",
    "https://stackoverflow.com/questions/43529931/how-to-calculate-prediction-uncertainty-using-keras  \n",
    "http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html#uncertainty-sense  \n",
    "https://www.reddit.com/r/deepfeec/comments/4bjwx7/plano_de_ataque_redes_neurais_bayesianas_resolvem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow function to apply dropout during prediction\n",
    "import keras.backend as K\n",
    "f = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(f, x, n_iter=200):\n",
    "    \"\"\"\n",
    "    All below computation based on couple variables:\n",
    "    l - prior length-scale, this captures our belief over the function frequency.\n",
    "        A short length-scale l corresponds to high frequency data, and a long length-scale\n",
    "        corresponds to low frequency data (choosen experimentally, I cannot confirm if this variable\n",
    "        is good for this problem)\n",
    "    N - number of training samples\n",
    "    w_d - weight_decay (originally it was model.weight_decay but keras sequential model does not have this attribute\n",
    "            and along with my understending it is just a regularization), in this situation we have Dropout\n",
    "            regularization so this variable is also set experimentally, there should be further explenation\n",
    "    \"\"\"\n",
    "    result = np.zeros((n_iter,) + x.shape)\n",
    "\n",
    "    for iter in range(n_iter):\n",
    "        result[iter] = f([np.asmatrix(x).reshape(2000,1), 1])[0][:,0]\n",
    "        pred_1 = list()\n",
    "        for i in range(len(result[iter])):\n",
    "            # make one-step forecast\n",
    "            X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "            # invert scaling\n",
    "            yhat = invert_scale(scaler, X, result[iter][i])\n",
    "            # invert differencing\n",
    "            yhat = inverse_difference(series, yhat, len(test_scaled)+1-i)\n",
    "            # store forecast\n",
    "            pred_1.append(yhat)\n",
    "        result[iter] = pred_1\n",
    "\n",
    "    prediction = result.mean(axis=0)\n",
    "    uncertainty = result.var(axis=0)\n",
    "    \n",
    "#     l = 1e2\n",
    "#     N = train_scaled.shape[0]\n",
    "#     w_d = 10\n",
    "#     tau = l**2 * (1 - 0.6) / (2 * N * w_d)\n",
    "#     uncertainty += tau**-1\n",
    "    \n",
    "    return prediction, uncertainty, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction part\n",
    "p, u, r = predict_with_uncertainty(f,test_scaled[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(series[-2000:-1], 'r') # true values\n",
    "plt.plot(p[1:], 'b') # predicted mean values\n",
    "plt.plot(p[1:]+1.96*u[1:], 'y')\n",
    "plt.plot(p[1:]-1.96*u[1:], 'g')\n",
    "plt.fill_between(x = range(1999), y1 = p[1:]+1.96*u[1:], y2=p[1:]-1.96*u[1:]) # predicted uncertanity interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.tm_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,1440,10)\n",
    "y = np.cos(np.linspace(0, 4*np.pi, 144))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(x,y+2)\n",
    "plt.xticks([0,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60,11*60,12*60,\n",
    "            13*60,14*60,15*60,16*60,17*60,18*60,19*60,20*60,21*60,22*60,23*60,24*60], \n",
    "           [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi day dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days = list()\n",
    "def preprocess_data():\n",
    "    for day in range(4,8):\n",
    "        data_in = pd.read_csv(\"/media/amadeusz/16A4DBE9A4DBC8FF/TF_NEW/Tensorflow-Bootcamp-master/test/2015-06-0{}/series_0{}_full_day_grouped_in.csv\".format(day,day))\n",
    "        data_out = pd.read_csv(\"/media/amadeusz/16A4DBE9A4DBC8FF/TF_NEW/Tensorflow-Bootcamp-master/test/2015-06-0{}/series_0{}_full_day_grouped_out.csv\".format(day,day))\n",
    "        data = data_in\n",
    "        data['obyt'] = data_out['ibyt']\n",
    "        data['ts'] = pd.to_datetime(data['ts'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "        data['ibyt'] = data['ibyt'].apply(lambda x: x*1/1e9)\n",
    "        data['obyt'] = data['obyt'].apply(lambda x: x*1/1e9)\n",
    "        data['over_byt'] = data['ibyt'] + data['obyt']\n",
    "        data = data.groupby(['ts']).sum()\n",
    "        data.index.name = None\n",
    "        data = data.groupby([data.index.hour, data.index.minute]).sum()\n",
    "        data['hours'] = data.index.labels[0]\n",
    "        data['minutes'] = data.index.labels[1]\n",
    "        days.append(data)\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(days[0].shape[0]*len(days))\n",
    "r = list()\n",
    "y_in = list()\n",
    "y_out = list()\n",
    "y_over = list()\n",
    "y_hours = list()\n",
    "y_minutes = list()\n",
    "for day in days:\n",
    "    r = r + day.index.levels[0].tolist()\n",
    "    y_in = y_in + day.groupby(level=[0,1]).sum()['ibyt'].tolist()\n",
    "    y_out = y_out + day.groupby(level=[0,1]).sum()['obyt'].tolist()\n",
    "    y_over = y_over + day.groupby(level=[0,1]).sum()['over_byt'].tolist()\n",
    "    y_hours = y_hours + day.groupby(level=[0,1]).sum()['hours'].tolist()\n",
    "    y_minutes = y_minutes + day.groupby(level=[0,1]).sum()['minutes'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data=np.asarray(y_in), columns=['in'])\n",
    "dataset['out'] = np.asarray(y_out)\n",
    "dataset['overall'] = np.asarray(y_over)\n",
    "dataset['hour'] = np.asarray(y_hours)\n",
    "dataset['minute'] = np.asarray(y_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(x, y_in, 'g', label='in_byt')\n",
    "plt.legend()\n",
    "plt.title('Bandwidth in AGH campus')\n",
    "plt.ylabel('Bandwidth [Gbps]')\n",
    "plt.xlabel('Hour')\n",
    "plt.xticks(range(0,days[0].shape[0]*len(days),120), r[::2])\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(x, y_out, 'y', label='out_byt')\n",
    "plt.legend()\n",
    "plt.title('Bandwidth in AGH campus')\n",
    "plt.ylabel('Bandwidth [Gbps]')\n",
    "plt.xlabel('Hour')\n",
    "plt.xticks(range(0,days[0].shape[0]*len(days),120), r[::2])\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(x, y_over, 'r', label='over_byt')\n",
    "plt.legend()\n",
    "plt.title('Bandwidth in AGH campus')\n",
    "plt.ylabel('Bandwidth [Gbps]')\n",
    "plt.xlabel('Hour')\n",
    "plt.xticks(range(0,days[0].shape[0]*len(days),120), r[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data, n_test, n_seq):\n",
    "    # split into dayes (one last day will be evaluating for tests)\n",
    "    train, test = data[0:-n_test], data[-n_test:]\n",
    "    # restructure into windows of 30 minutes data\n",
    "    train = array(np.split(train, len(train)/n_seq))\n",
    "    test = array(np.split(test, len(test)/n_seq))\n",
    "    try:\n",
    "        return train.reshape(train.shape[0], train.shape[1], data.shape[1]), test.reshape(test.shape[0], test.shape[1], data.shape[1])\n",
    "    except IndexError as e:\n",
    "        return train.reshape(train.shape[0], train.shape[1], 1), test.reshape(test.shape[0], test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 30*40\n",
    "n_seq = 30\n",
    "# load dataset\n",
    "# series = dataset['overall'].values\n",
    "series = np.asarray(difference(dataset['overall'].values, 5)[n_seq-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_dataset(series, n_test, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape # 144 samples, 30 minutes long, 1 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape # 144 samples, 30 minutes long, 5 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To supervised**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=30):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end < len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_lag = 60 # based on 30 previous samples (minutes)\n",
    "# n_seq = 1 # predict 30 proceeding samples (minutes)\n",
    "train_X, train_Y = to_supervised(train, n_lag, n_seq) # X-previous 30 minutes, Y-proceeding 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, n_lag, n_seq):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_lag, n_seq)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 70, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model(train, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_lag):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_lag:, 0]\n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_lag):\n",
    "    # fit model\n",
    "#     model = build_model(train, n_lag)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_lag)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores, predictions = evaluate_model(train, test, n_lag)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "minutes = range(n_lag)\n",
    "pyplot.plot(minutes, scores, marker='o', label='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_lag), test[0,:,0])\n",
    "plt.plot(range(n_lag), predictions[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder-decoder LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "# train the model\n",
    "def build_model(train, n_lag, n_seq):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_lag, n_seq)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 20, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model(train, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores, predictions = evaluate_model(train, test, n_lag)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "minutes = range(n_lag)\n",
    "pyplot.plot(minutes, scores, marker='o', label='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_lag), test[0,:,0])\n",
    "plt.plot(range(n_lag), predictions[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generowanie predykcji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, range_predict, n_lag):\n",
    "    # fit model\n",
    "#     model = build_model(train, n_lag)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(range_predict):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_lag)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "#         history.append(test[i, :])\n",
    "#         history.append(yhat_sequence) # predict 30 min and append to history, based on predicted try predict another\n",
    "#         print(test[i, :].shape)\n",
    "        history.append(np.append(history[-1][1:], np.mean(yhat_sequence)).reshape(30,1))\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "#     score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return predictions, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, history = evaluate_model(train, 1440, n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "n_lag = 60\n",
    "n_set = 1\n",
    "# load dataset\n",
    "data = dataset\n",
    "values = data.values\n",
    "# integer encode direction\n",
    "# encoder = LabelEncoder()\n",
    "# values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_lag, n_set)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[n_lag*data.shape[1],\n",
    "                                n_lag*data.shape[1]+1,\n",
    "                                n_lag*data.shape[1]+3,\n",
    "                                n_lag*data.shape[1]+4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(dataset['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(dataset['overall'], lags=50)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return diff\n",
    " \n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, value):\n",
    "    return value + last_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataset with a linear trend\n",
    "f, axs = plt.subplots(3,2,figsize=(20,7))\n",
    "axs[0][0].plot(dataset['in'])\n",
    "axs[0][0].set_title('in')\n",
    "axs[0][1].plot(dataset['out'])\n",
    "axs[0][1].set_title('out')\n",
    "axs[1][0].plot(dataset['overall'])\n",
    "axs[1][0].set_title('overall')\n",
    "axs[1][1].plot(dataset['hour'])\n",
    "axs[1][1].set_title('hour')\n",
    "axs[2][1].plot(dataset['minute'])\n",
    "axs[2][1].set_title('minute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference the dataset\n",
    "diff = difference(data, 5)\n",
    "pyplot.plot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the difference\n",
    "inverted = [inverse_difference(data[i], diff[i]) for i in range(len(diff))]\n",
    "pyplot.plot(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lag = 60\n",
    "n_set = 1\n",
    "# load dataset\n",
    "values = np.asarray(diff).reshape(len(diff),1)\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_lag, n_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_test = 1440\n",
    "train = values[:-n_test, :]\n",
    "test = values[-n_test:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(200))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    " \n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data, n_test, n_seq):\n",
    "    # split into dayes (one last day will be evaluating for tests)\n",
    "    train, test = data[0:-n_test], data[-n_test:]\n",
    "    # restructure into windows of 30 minutes data\n",
    "    train = array(np.split(train, len(train)/n_seq))\n",
    "    test = array(np.split(test, len(test)/n_seq))\n",
    "    try:\n",
    "        return train.reshape(train.shape[0], train.shape[1], data.shape[1]), test.reshape(test.shape[0], test.shape[1], data.shape[1])\n",
    "    except IndexError as e:\n",
    "        return train.reshape(train.shape[0], train.shape[1], 1), test.reshape(test.shape[0], test.shape[1], 1)\n",
    " \n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    " \n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    " \n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    " \n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_lag, n_seq):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_lag, n_seq)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 1, 7, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    " \n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    " \n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "# \tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "# \tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn predictions#score, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit_transform(values)\n",
    "diff = pd.DataFrame(data=scaler.fit_transform(np.asarray(difference(dataset['in'], 5)).reshape(dataset.shape[0]-5,1)), columns=['in'])\n",
    "diff['out'] = scaler.fit_transform(np.asarray(difference(dataset['out'], 5)).reshape(dataset.shape[0]-5,1))\n",
    "diff['overall'] = scaler.fit_transform(np.asarray(difference(dataset['overall'], 5)).reshape(dataset.shape[0]-5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_input = 60\n",
    "n_lag = 60\n",
    "n_seq = 30\n",
    "n_test = 1440//2\n",
    "# load the new file\n",
    "data = diff.tail(diff.shape[0]-n_input+5)\n",
    "# split into train and test\n",
    "train, test = split_dataset(data.values, n_test, 30)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(train, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = evaluate_model(train, test, n_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(30), predict[0])\n",
    "plt.plot(range(30), test[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_inv = scaler.inverse_transform(predict[0].reshape(len(predict[0]), 1))\n",
    "test_inv = scaler.inverse_transform(test[0,:,0].reshape(len(test[0,:,0]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_inverted = [inverse_difference(data[i], pre_inv[i]) for i in range(len(pre_inv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inverted = [inverse_difference(data[i], diff[i]) for i in range(len(diff))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.DataFrame(diff)\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "#     return diff_values\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    print(supervised.shape)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test, supervised\n",
    "\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=n_lag, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(16, kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(8, kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(n_seq, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "\n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "\n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "# # plot the forecasts in the context of the original dataset\n",
    "# def plot_forecasts(series, forecasts, n_test):\n",
    "#     # plot the entire dataset in blue\n",
    "#     plt.figure(figsize=(15,8))\n",
    "#     plt.plot(series)\n",
    "#     # plot the forecasts in red\n",
    "#     for i in range(len(forecasts)):\n",
    "#         off_s = len(series) - n_test + i - 1\n",
    "#         off_e = off_s + len(forecasts[i]) + 1\n",
    "#         xaxis = [x for x in range(off_s, off_e)]\n",
    "#         yaxis = [series[off_s]] + forecasts[i]\n",
    "#         yaxis = np.mean(forecasts[i])\n",
    "#         plt.plot(xaxis, yaxis, color='red')\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(series)\n",
    "    # plot the forecasts in red\n",
    "    fore = list()\n",
    "    for i in forecasts:\n",
    "        fore.append(np.mean(i))\n",
    "    for i in range(len(fore)):\n",
    "        xaxis = len(series) - n_test + i\n",
    "        yaxis = fore[i]\n",
    "        plt.plot(xaxis, yaxis, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "series = dataset['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lag = 1440\n",
    "n_seq = 1\n",
    "n_test = 1\n",
    "n_epochs = 50\n",
    "n_batch = 10\n",
    "scaler, train, test, s = prepare_data(series, n_test, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wider_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:,-n_seq:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train[:,0:-n_seq], train[:,-n_seq:], epochs=n_epochs, batch_size=n_batch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:,0:-n_seq].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(model.predict(test[:,0:-n_seq], batch_size=n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(test[:,2:-n_seq], [1,2][:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecasts = list()\n",
    "for i in range(1440):\n",
    "    if i == 0:\n",
    "        predicted = model.predict(test[:,i:-n_seq], batch_size=n_batch)\n",
    "    else:\n",
    "        predicted = model.predict(np.append(test[:,i:-n_seq], forecasts[:i]).reshape(1,1440), batch_size=n_batch)\n",
    "#         predicted = np.mean(model.predict(np.asarray([forecasts[i-n_lag:]]).reshape(1,n_lag), \n",
    "#                                           batch_size=n_batch))\n",
    "    forecasts.append(predicted)\n",
    "forecasts = np.asarray([forecasts]).reshape(1,1440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.predict(test[:,0:-n_seq], batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "# actual = [row[-n_seq:] for row in test]\n",
    "# actual = inverse_transform(series, actual, scaler, n_test+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_forecasts(test[:,0:-n_seq], forecast, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "# plt.plot(actual[0])\n",
    "plt.plot(forecast[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "# plt.plot(actual[0])\n",
    "plt.plot(forecast[0][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "series = dataset['overall'].values\n",
    "# configure\n",
    "n_lag = 1000\n",
    "n_seq = 60\n",
    "n_test = 1060\n",
    "n_epochs = 5\n",
    "n_batch = 32\n",
    "n_neurons = 64\n",
    "# prepare data\n",
    "# x = prepare_data(series, n_test, n_lag, n_seq)\n",
    "scaler, train, test, supervised = prepare_data(series, n_test, n_lag, n_seq)\n",
    "# fit model\n",
    "model = wider_model()\n",
    "# # evaluate model with standardized dataset\n",
    "history = model.fit(train[:,0:-n_seq], train[:,-n_seq:], epochs=n_epochs, batch_size=n_batch, verbose=1)\n",
    "# history = model.fit(train[0:-n_seq], train[-n_seq:], epochs=n_epochs, batch_size=n_batch, verbose=1)\n",
    "# # Predict test dataset without confidence interval\n",
    "forecasts = model.predict(test[:,0:-n_seq], batch_size=n_batch)\n",
    "# forecasts = model.predict(test[0:-n_seq], batch_size=n_batch)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[-n_seq:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# # plot forecasts\n",
    "# plot_forecasts(series, forecasts, n_test+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series[-n_test:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1,0:-n_seq].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(test[-1,1:-n_seq], [1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([1,2,3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore = list()\n",
    "for i in range(1440*2):\n",
    "    forecasts = model.predict(np.append(test[-1,i:-n_seq], fore), batch_size=n_batch)\n",
    "    fore.append(np.mean(forecasts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.predict(test[:,0:-n_seq], batch_size=n_batch)\n",
    "# forecasts = model.predict(test[0:-n_seq], batch_size=n_batch)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test)\n",
    "actual = [row[-n_seq:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(series[-n_test-1:-n_test+9], 'b-')\n",
    "z = list()\n",
    "for i in forecasts:\n",
    "    z.append(np.mean(i))\n",
    "# plt.plot(forecasts[0])\n",
    "# plt.plot(forecasts[2])\n",
    "plt.plot(z[:10], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[:,-n_seq:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecasts = model.predict(test[0:-n_seq], batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(series, forecasts, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(series, forecasts, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = list()\n",
    "fore = list()\n",
    "for i, (ac, fo) in enumerate(zip(actual, forecasts)):\n",
    "    acc.append(np.mean(ac))\n",
    "    fore.append(np.mean(fo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['overall'].values[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "# plt.plot(acc)\n",
    "plt.plot(dataset['overall'].values[-n_test:])\n",
    "plt.plot(fore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale values to -1, 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_values = scaler.fit_transform(x)\n",
    "\n",
    "# scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "# # transform into supervised learning problem X, y\n",
    "# supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "# supervised_values = supervised.values\n",
    "# # split into train and test sets\n",
    "# train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "# return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[0:-1440], data[-1440:]\n",
    "\t# restructure into windows of weekly data\n",
    "# \ttrain = array(split(train, len(train)/7))\n",
    "# \ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 70, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, 0]\n",
    "\t# reshape into [1, n_input, 1]\n",
    "\tinput_x = input_x.reshape((1, len(input_x), 1))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "# dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "# n_input = 7\n",
    "# score, scores = evaluate_model(train, test, n_input)\n",
    "# # summarize scores\n",
    "# summarize_scores('lstm', score, scores)\n",
    "# # plot scores\n",
    "# days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "# pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = series_to_supervised(train, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One day dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = pd.read_csv(\"/media/amadeusz/16A4DBE9A4DBC8FF/TF_NEW/Tensorflow-Bootcamp-master/test/2015-06-09/series_09_full_day_grouped_in.csv\")\n",
    "data_out = pd.read_csv(\"/media/amadeusz/16A4DBE9A4DBC8FF/TF_NEW/Tensorflow-Bootcamp-master/test/2015-06-09/series_09_full_day_grouped_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_in\n",
    "data['obyt'] = data_out['ibyt']\n",
    "data['ts'] = pd.to_datetime(data['ts'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "data = data.groupby(['ts']).sum()\n",
    "data.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby([data.index.hour, data.index.minute]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(data.shape[0])\n",
    "y_in = data.groupby(level=[0,1]).sum()['ibyt']\n",
    "y_out = data.groupby(level=[0,1]).sum()['obyt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(x, y_in, label='in_byt')\n",
    "plt.plot(x, y_out, label='out_byt')\n",
    "plt.plot(x, y_over, label='over_byt')\n",
    "plt.legend()\n",
    "plt.title('Bandwidth in AGH campus')\n",
    "plt.ylabel('Bandwidth [Gbps]')\n",
    "plt.xlabel('Hour')\n",
    "# plt.xticks(range(0,data.shape[0],60),\n",
    "#            [i//60 for i in range(0,data.shape[0],60)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time'] = pd.to_datetime(data['Timestamp'], format='%d/%m/%Y%H:%M:%S', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['date'] = pd.to_datetime(data['start_time'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=['time'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['time'].apply(lambda x: x.month)\n",
    "data['day'] = data['time'].apply(lambda x: x.day)\n",
    "data['hour'] = data['time'].apply(lambda x: float(\"{}.{}\".format(x.hour, x.minute)))\n",
    "data['minute'] = data['time'].apply(lambda x: x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_range = list(set(data['month']))\n",
    "month_range.sort()\n",
    "print(\"Month: {}\".format(month_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_range = list(set(data['day']))\n",
    "day_range.sort()\n",
    "print(\"Days: {}\".format(day_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = dict()\n",
    "for month in month_range:\n",
    "    grouped_data[month] = dict()\n",
    "    for day in day_range:\n",
    "        grouped_data[month][day] = data.loc[(data['day'] == day) & (data['month'] == month)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data[4].keys()bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = each_day_data[12].groupby(['hour']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = grouped.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = grouped['num_bytes'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for month in grouped_data.keys():\n",
    "    for day in grouped_data[month].keys():\n",
    "        if grouped_data[month][day].shape[0] != 0:\n",
    "            grouped = grouped_data[month][day].groupby(['hour']).sum()\n",
    "            x = grouped.index.values\n",
    "            y = (grouped['Total.Length.of.Fwd.Packets'] / grouped['Flow.Duration']).values\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(x,y)\n",
    "            plt.title('Day {}'.format(day))\n",
    "            plt.ylabel('Bandwidth [Mbps]')\n",
    "            plt.xlabel('Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2] + [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2]) / pd.Series([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
